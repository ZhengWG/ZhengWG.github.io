---
layout: post
title: AVOD论文解读
date: 2019-10-13 00:56:20.000000000 +09:00
categories: [算法篇]
tags: [Paper_Reading, CV, 3D]
---

# 目录

1.  [前言](#org2bf16eb)
2.  [论文基本信息](#org985bb71)
3.  [动机和思路](#orgd5f326c)
4.  [算法基本流程](#org989a304)
5.  [具体实验分析](#orgf1dde18)
6.  [个人总结](#org961c653)


<a id="org2bf16eb"></a>

# 前言

AVOD作为基于Camera信息和Lidar信息融合的3D detection算法，是目前为数不多的开源的SOTA算法之一，相关资料如下：

[KITTI榜单](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev)

[AVOD论文](https://arxiv.org/pdf/1712.02294.pdf)

[AVOD源码](https://github.com/kujason/avod)

[AVOD源码解读](https://blog.csdn.net/sunny0660/article/details/104078746)


<a id="org985bb71"></a>

# 论文基本信息

一作信息：

Jason Ku 多伦多大学在校研究生，从事无人驾驶感知算法研究

其他相关工作：

`IROS 2019`

`Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation`

Kitti行人检测中排名第5  Lidar+Camera前融合方案

`CVPR 2019`

`Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction`

单目3D目标检测
KITTI当前SOTA:  `Car` 排名112 `Pedestrain` 排名44 `Cyclist` 排名38

引用：175

通讯作者：

`Steven L. Waslander`
多伦多大学副教授,研究领域：`Unmanned Aerial Vehicles`，`SLAM`，`Object Detection`，`Motion Planning`

引用：4741


<a id="orgd5f326c"></a>

# 动机和思路

-   无人驾驶Preception当前Fusion的三种思路：
    -   Image based:
    以 `F-PointNet` 为代表的先用Camera做2D检测，再投影到3D空间内对视锥进行特征提取，再进行检测。特征是级联结构，性能受2D检测制约。`Deep Manta` ，Camera预测深度，3D定位性能差。
    -   `BEV`  （Bird’s eye view）based: 以`MV3D` 为代表的将点云投射在`BEV` 上生成`proposal` ，再通过`ROI crop` 出Camera的相应区域做Fusion去修正预测框。特征是对高度信息运用较少，可能损失了一部分3D信息。
    -   3D based: 直接在3D空间中做融合，因为这个方向上融合比较困难，所以目前相关文章较少。特征是相对计算量较大。
-   `BEV`  based代表作—`MV3D` : `MV3D` 的基本思路是先通过`BEV` 视角下得到`3D Proposal` ，然后分别投影到FV视角和camera视角得到各自图像上的`ROI` 然后进行三类feature的融合，最后在融合的feature上进行最终Box的分类和回归操作。
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_000608.png)

-   不足之处：
    -   `backbone` 特征提取的过程中，下采样会导致小物体的信息不可获取：比如行人在`BEV` 图上为`8\*6pixel` ，经过三次`downsampling` 后，pixel<1，信息丢失，所以小物体检测不好
    -   采用的八个顶点`Encoding` 方式，缺少长方体的`Constraint` ，未具体说明`orientation` 的回归，（`AVOD` 中提到`MV3D` 采用的方向判定为与长方体平面的长边方向一致），容易上下颠倒:
        ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_000638.png)


<a id="org989a304"></a>

# 算法基本流程

-   流程图:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_000707.png)

-   主要改进：
    -   `feature extractor` 借鉴`FPN` 结构，融合浅层和深层feature，形成 `high-resolution feature map`
    -   `crop and resize` :将生成的`3D proposal` 投影到各自`plane` 后通过`bilinearly resized` 到`n\*n feature map` :
        ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_001451.png)

    -   采用约束的`box coding` 方式：缩小了参数维度：`24d->10d` ，回归了平面旋转向量:
        ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_000935.png)

-   其他细节：
    -   `BEV` 为6维feature：5 heights(5 equal slices on Z axis)+ `density information`
    -   `1\*1Conv` ：`RPN` 网络输出`anchors` 数量很多，输出feature map维度过高，需要大量显存，通过1X1网络进行降维
    -   `3D Proposal Generatiuon` ：`background box` 不作为regression的loss计算。car的iou阈值略高于其他两类(0.5→0.45)。NMS阈值为0.8。Training proposal数目1024，Infereance 300
    -   `Second Stage Detection Network` :取与回归得到向量最接近的向量为orientation（预先选定了4个方向，根据corner位置）


<a id="orgf1dde18"></a>

# 具体实验分析

-   评价指标:
    -   `3D AP` : `3D IOU` ：摄像机坐标系下两3d长方体相交的体积占两长方体总体积的比例
    -   `BEV AP` : `2D IOU` :摄像机坐标系下的物体投影到地面上的`overlap` ，即`bird eye view` 下两box交集占两box总面积比例
    -   `AHS` (Average Heading Similarity): `3D IOU and global orientation angle`
-   `KITTI` :测试集性能（无法复现）上看，`AVOD` 在Cyclist性能略差于`F-PointNet` ，Pedestrian各有胜负，car性能更好，但是速度更快:

![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_001325.png)

-   `RPN Recall` :验证集性能，RPN Recall高于其他同类算法，对于car一类提升不明显，可能因为本身car尺寸大，特征也比较比较明显，性能本身处于一个较高的level。另外MV3D没有对照数据:

![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_001548.png)

-   `AHS` :验证集性能(3D AP)，朝向性能明显更好:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_001235.png)

-   `Ablation Study` (消融实验):Base Network为参照网络：采用vgg-net作为backbone的VOD网络。验证集性能(3D AP)，`BEV Image` 融合和`Feature Pyramid Extractor` 在car类上提升不明显，在Pedestrian和Cyclist上提升明显。但是这一点在复现过程中由于只能利用train-split集的小样本（Pedestrian和Cyclist本身数目不足）以及path drop的使用，可能会导致抖动较大，论文中也提到目前没有论文公开在验证集上的Pedestrian和Cyclist类的性能。

![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2019-10-13-AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2019_10_13_Joint_3D_Proposal_Generation_and_Object_Detection_from_View_Aggregation_20210710_001157.png)


<a id="org961c653"></a>

# 个人总结

-   优点：相对于前作MV3D，做得更加精细，主要体现在：
    -   `FPN` 多层特征的融合
    -   `3D Box orientation` 的回归策略
    -   `Crop and Resize` 的采用
-   不足之处：
    -   plane平面的提取（用于生成BEV图），性能依赖于点云的密集成都
    -   生成Proposal的过程中Image和Lidar数据是分离的，基于`ROI Fusion` 已处于高层feature的fusion，参考`MMF` （`Multi-Task Multi-Sensor Fusion for 3D Object Detection` ）实现了`roi-wise` 和`point-wise` 上的fusion，性能更好（代码未开源）
    -   `ROI-Fusion` 的过程中存在`Image feature` 和`BEV feature` 不对齐的问题
    -   基于BEV视图下能够得到第一阶段较高的Recall，但是后期3D位置的refinement，通过原始3d的点云信息更准确(但是要解决原始点云数据稀疏分布的问题，如通过采样，聚类等方式实现key area的融合)
