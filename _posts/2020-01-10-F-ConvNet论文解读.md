---
layout: post
title: F-ConvNet论文解读
date: 2020-01-10 00:56:20.000000000 +09:00
categories: [算法篇]
tags: [Paper_Reading, CV, 3D]
---

# 目录

1.  [论文背景以及基本思想](#org4cb91b3)
2.  [算法基本流程](#org25493ae)
3.  [算法细节](#org5d8aebd)

[Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection](https://arxiv.org/pdf/1903.01864v1.pdf)


<a id="org4cb91b3"></a>

# 论文背景以及基本思想

论文出发点:本文基于F-PointNet的思想，认为F-PointNet非端到端，最终3D Box的预测强依赖于3D点云的分割结果，前置模块的点云缺失会造成后期预测不准确。


<a id="org25493ae"></a>

# 算法基本流程

-   核心点:
    -   将2D detection的结果得到的视锥等分为T个部分，对所有视锥部分进行PointNet特征的提取，组成2D的feature map（L\*d）通过FCN（kernel:3\*d）网络进行每个部分的box分类和位置预测
    -   视锥分割采取多组stride，在网络不同阶段进行多尺度的融合
-   整体框架图:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2020-01-10-F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2020_01_04_F-ConvNet_20210710_002603.png)

-   视锥的划分，通过不同stride产生的feature进行融合:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2020-01-10-F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2020_01_04_F-ConvNet_20210710_002618.png)

-   多尺度进行feature的融合:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2020-01-10-F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2020_01_04_F-ConvNet_20210710_002740.png)


<a id="org5d8aebd"></a>

# 算法细节

其他细节:

-   采用Focal loss平衡foreground和background
-   Final refinement：在得到3D box的输出后，将预测得到的3D box进行expand和normalization（将框扩大factor倍（1.2），方向不变），然后将这些框内的points再次作为网络的输入，进行二次的refinement，结果显示refinement的提升很大（+2Ap以上）:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2020-01-10-F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2020_01_04_F-ConvNet_20210710_002756.png)

-   消融实验:focal loss和refinement:
    ![img](https://github.com/ZhengWG/Imgs_blog/raw/master/2020-01-10-F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/2020_01_04_F-ConvNet_20210710_002823.png)

