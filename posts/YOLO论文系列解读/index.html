<!DOCTYPE html><html lang="zh-Hans" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="YOLO论文系列解读" /><meta property="og:locale" content="zh_Hans" /><meta name="description" content="目录" /><meta property="og:description" content="目录" /><link rel="canonical" href="https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/" /><meta property="og:url" content="https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/" /><meta property="og:site_name" content="Johney Zheng" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2019-11-13T23:56:20+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="YOLO论文系列解读" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"目录","url":"https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/","headline":"YOLO论文系列解读","dateModified":"2021-08-21T14:06:13+08:00","datePublished":"2019-11-13T23:56:20+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/"},"@context":"https://schema.org"}</script><title>YOLO论文系列解读 | Johney Zheng</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Johney Zheng"><meta name="application-name" content="Johney Zheng"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-200658716-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-200658716-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Johney Zheng</a></div><div class="site-subtitle font-italic">Johney Zheng的小站</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/ZhengWG" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['21625111','zju.edu.cn'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>YOLO论文系列解读</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>YOLO论文系列解读</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Johney Zheng </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Nov 13, 2019, 11:56 PM +0800" prep="on" > Nov 13, 2019 <i class="unloaded">2019-11-13T23:56:20+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sat, Aug 21, 2021, 2:06 PM +0800" prefix="Updated " > Aug 21, 2021 <i class="unloaded">2021-08-21T14:06:13+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2416 words">13 min</span></div></div><div class="post-content"><h1 id="目录">目录</h1><ol><li><a href="#org21296b6">前言</a><li><a href="#orgd06f330">论文基本信息</a><li><a href="#org02246b9">论文出发点和思路</a><li><a href="#org1c91967">算法基本流程</a><li><a href="#org8136767">具体实验分析</a><li><a href="#org9421cbe">YOLOv1个人总结</a><li><a href="#org1210da1">改进-YOLOv2</a><ol><li><a href="#org12ccb89">出发点</a><li><a href="#org59cd54b">改进方案</a><li><a href="#orgd9d9db5">改进结果</a></ol><li><a href="#org1c82c5a">改进-YOLOv3</a><ol><li><a href="#orgeb1e145">出发点</a><li><a href="#org6c0b5b5">改进方案</a><li><a href="#org8fbd907">改进结果</a></ol></ol><p><a id="org21296b6"></a></p><h1 id="前言">前言</h1><p>YOLO作为最早的One-stage算法框架，实现了保持较好性能的前提下保证了模型较快的速度和轻便的性能。从Yolov1-&gt;Yolov3，三个版本的迭代也可以很好地观察作者进行性能提升的思路和方法。</p><p><a id="orgd06f330"></a></p><h1 id="论文基本信息">论文基本信息</h1><p>作者信息:Joseph Redmon,华盛顿大学phD，YOLOv1-v3作者，相关研究还有Xnor-net等。</p><p><a href="https://pjreddie.com/darknet/yolo/">YOLO官网</a></p><p><a href="https://arxiv.org/abs/1506.02640">YOLOv1_Paper</a></p><p><a href="https://arxiv.org/abs/1612.08242">YOLOv2_Paper</a></p><p><a href="https://arxiv.org/abs/1804.02767">YOLOv3_paper</a></p><p><a id="org02246b9"></a></p><h1 id="论文出发点和思路">论文出发点和思路</h1><p>YOLOv1设计的出发点建立于人本身对图片的认知本身快速与准确的：只需要对整张图片扫描一次即可快速获取物体的类别与位置信息:You Only Look Once。其他主流物体检测算法，如DPM（defaormable parts models）通过sliding windows的方式，通过在每个spaced local位置设置分类器实现检测任务。RCNN系列通过two-stage的方式，且RCNN和Fast RCNN是无法实现端到端训练的。</p><p>YOLOv1首次通过One-Stage的方式实现Object Detection的任务，将整个任务作为bounding box的位置和类别的回归任务。下图可以简单描述YOLOv1的工作原理，YOLOV1通过直接在全图上利用卷积网络实现位置和类别的预测。这种方案的优势在于：速度快，不需要复杂的pipeline设计，标准的YOLO检测速度可以达到45FPS，Fast YOLO可以达到155FPS，当时的YOLO的检测mAP可以达到其他实时系统的两倍以上；YOLO能够更好地利用global的信息，减少背景错误；YOLO的泛化性更强。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_233721.png" alt="img" /></p><p><a id="org1c91967"></a></p><h1 id="算法基本流程">算法基本流程</h1><p>YOLOv1：</p><ul><li>Unified Detection:</ul><p>YOLO将图片分为S*S的栅格，每个栅格对中心落在栅格内部的物体负责。每个栅格会预测B个bounding box，confidence信息和位置信息:x,y,w,h。其中confidence信息表示为条件概率的形式:<code class="language-plaintext highlighter-rouge">confidence=Pr(Object)\*IOU</code> ，即如果bounding box预测无物体，则<code class="language-plaintext highlighter-rouge">Pr(object)=0</code> ，则confidence为0，如果预测有物体，则confidence为预测box和gt的IOU。其中,x,y表示预测的bounding box的中心与栅格边界的相对位置，w，h表示为bounding box的width,height相对于整幅图像的比例。另外每个grid cell还会预测c类的<code class="language-plaintext highlighter-rouge">conditional confidence:Pr(Class_i|Object)</code> ,infer过程中通过<code class="language-plaintext highlighter-rouge">Pr(Class_i|Object)\*Pr(Object)\*IOU</code> 来作为bounding box的类预测confidence。注意这里的class confidence只针对c个类，而bounding box的confidence则针对每个box。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_233759.png" alt="img" /></p><ul><li>Network Design:</ul><p>YOLO的网络借鉴了GoogleNet，通过1*1和3*3的卷积层替代了Inception结构。标准的YOLO网络有24层卷积层，后接两层全连接层，而Fast Yolo则只有9层卷积层。YOLO的输出tensor大小为7*7*30。与prediction的siz相匹配。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_233834.png" alt="img" /></p><ul><li>Training:</ul><p>先在ImageNet 1000-class分类任务上的进行网络的Pretrain，使用上述网络中的前20层卷积层， 后接一个average-pooling层和一个全连接层。将Pretrain得到的前20层全连接层作为Detection网络的前置网络，并加入后续的4层卷积层以及两个全连接层，最后层预测得到<code class="language-plaintext highlighter-rouge">class probility</code> 以及<code class="language-plaintext highlighter-rouge">bounding box coordinates</code> 。其中w,h,x,y需要归一化到0-1，以保证w,h小于图片尺寸，且位置在特定的grid cell边界范围内。</p><ul><li>Loss设计 Loss采用sum-squared error loss，但是对不同类的loss采用了不同的权重设计：<ul><li>coordinates error需要有更高的权重<li>no-object的栅格数目比重很大，其所占的loss很大，需要降低no-object的loss权重，降低对网络的贡献率<li>大物体小物体的误差容忍率应该是不一致的，小物体偏移对IOU影响更大，作者采用原本height和width的平方根代替原始值</ul></ul><p>训练过程中，可能存在多个box预测同一个物体，则只取IOU最大的predictor作最终的预测，最终的loss设计如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_233902.png" alt="img" /></p><p><a id="org8136767"></a></p><h1 id="具体实验分析">具体实验分析</h1><ul><li>PASCAL VOC 2007实验结果：Faster-RCNN VGG-16的mAP高于YOLO，但是速度是YOLO的6倍多，而Faster-RCNN ZF速度慢于YOLO，但是mAP也小于YOLO。</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_233945.png" alt="img" /></p><ul><li>VOC2007错误分析：与Fast RCNN进行比较，可见YOLO的位置定位误差较大，但是background的误差小。因为本身YOLO采用grid cell进行预测位置的方式，因为损失了较多的细节信息，造成了位置定位不准。</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234015.png" alt="img" /></p><ul><li>VOC 2012:YOLOv1的性能与RCNN-VGG性能相当，主要是在小物体上的性能不如RCNN</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234048.png" alt="img" /></p><p><a id="org9421cbe"></a></p><h1 id="yolov1个人总结">YOLOv1个人总结</h1><p>YOLOv1的缺陷也是比较明显的：</p><ul><li>每个grid cell只预测2个bounding box，且不同cell之间的预测为互斥关系，所以对于相近的物体预测效果不好。<li>default box的尺寸需要从训练数据中学习得到，强依赖于训练数据的物体尺寸，且没有先验的尺寸，网络学习难度变大。<li>网络最后的输出feature为7*7尺寸，小物体信息损失严重，小物体检测效果差。</ul><p><a id="org1210da1"></a></p><h1 id="改进-yolov2">改进-YOLOv2</h1><p><a id="org12ccb89"></a></p><h2 id="出发点">出发点</h2><p>YOLOv2(YOLO9000),从三个维度进行YOLOv1的改进：<code class="language-plaintext highlighter-rouge">Better,Faster,Stronger</code> 。其中，Stronger主要利用分类结果进行训练，从而能够检测更多类别。</p><p>本文重点关注前两项优化内容。YOLOv1的两个主要缺陷为：大量的localization error和相对proposal-based方案的较低的recall。常见的提高detector的性能的方案是采用更大，更深的网络结构，但是这会影响模型的速度。所以作者没有单纯地加宽加深网络结构，而是采取了更好的representation，而让网络更好的学习（主要是Box和对应loss的设计），同时也采用了一系列tricks以及修掉原本YOLOv1本身存在的一些问题。</p><p><a id="org59cd54b"></a></p><h2 id="改进方案">改进方案</h2><p>作者的改进方案如下：</p><ul><li>BN层：在对所有卷积层加入BN层后，提升了2%的mAP<li>High Resolution Classifier:YOLOv1训练分类网络时采用的input size为224X224，但是detection网络采用的输入为448X448，意味着detection网络训练的过程中需要重新学习更大的输入尺寸。YOLOv2先将分类网络在448X448的输入下进行finetune,然后进行detection网络的finetune，提升了4%的mAP<li>Convolutional With Anchor Boxes:采用anchor box的方式进行bounding box的预测。首先移除了YOLOv1的全连接层以卷积层替代，移除最后的pooling层，增大输出的size，输入的图片size由448X448-&gt;416X416,目的是为了保持输出为13X13的奇数，以保留图片正中间的grid cell位置（作者认为这对图片中间的大物体预测有帮助）。另外box的class的预测仍然沿用之前的YOLOV1的策略。该方案的收益是提高了7%的recall，但是accuracy下降:69.5mAP@81%recall-&gt;69.2mAP@88%recall。<li><p>Dimension Clusters:采用K-means进行box的聚类，相同情况下，能够比hand picked得到的box有更高的Avg IOU。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234131.png" alt="img" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234145.png" alt="img" /></p><li>Direct location prediction:默认的bounding box的center point的预测方案：</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234224.png" alt="img" /></p><p>该方案的问题是偏移的最大值与anchor box的大小有关，没有其他约束，导致center点的位置有可能到图像中的任意位置，导致模型训练不稳定。所以作者沿用原先YOLO的预测位置的方式：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234241.png" alt="img" /></p><p>通过Dimension Clusters和Direct location prediction的方式，提高了anchor-version 5% mAP,示意图如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234735.png" alt="img" /></p><ul><li>Fine-Grained Features:通过pass-through（stacking adjacent features into diffrent channels）的方式将前置的feature层（26X26）与原先的feature map进行concat，融合多尺度性能，能够提高1% mAP。<li>Multi-Scale Training:为了让YOLOv2对input_size有更强的鲁棒性，模型训练过程中每10个batches会选择一个新的image size输入:{320,352,…,608}。<li>Faster:设计了<code class="language-plaintext highlighter-rouge">Darknet-19</code> : 5.58 billion operation, <code class="language-plaintext highlighter-rouge">72.9% top1 accuracy</code> , <code class="language-plaintext highlighter-rouge">91.2% top5 accuracy</code> on ImageNet.:</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234833.png" alt="img" /></p><p><a id="orgd9d9db5"></a></p><h2 id="改进结果">改进结果</h2><p>YOLOv1-&gt;YOLOv2改进： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234925.png" alt="img" /></p><p>PASCAL_VOC2007:实现了速度和性能良好的trade-off： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_234948.png" alt="img" /></p><p>PASCAL_VOC2012性能比较：性能基本一致，但是YOLOv2更快： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_235936.png" alt="img" /></p><p>COCO性能比较：COCO小物体更多，IOU=0.5下，性能与Faster-RCNN和SSD300基本相当，但是速度更快： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_235959.png" alt="img" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210709_235050.png" alt="img" /></p><p><a id="org1c82c5a"></a></p><h1 id="改进-yolov3">改进-YOLOv3</h1><p><a id="orgeb1e145"></a></p><h2 id="出发点-1">出发点</h2><p>原先YOLOv2的没有结合多尺度特征进行预测，pass-through的方案虽然实现了浅层特征的融合，但是也改变了特征的空间分布。YOLOv2的backbone-darknet19还有提升的空间。</p><p><a id="org6c0b5b5"></a></p><h2 id="改进方案-1">改进方案</h2><p>YOLOv3的改进更多地借鉴了诸如SSD，Faster-rcnn，FPN的优点，主要改进如下：</p><ul><li>Predictions Across Scales：参照FPN的设计，对原本13X13的输出feature进行上采样，分别得到26X26和52X52尺寸的feature，并与浅层的featue进行concat，最后通过卷积层进行预测。K-means采用的cluster数目为9。<li>Feature Extractor：加深了backbone的层数，并且引入Resnet的残差结构，实现了低运算量下较好的性能：</ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210710_000059.png" alt="img" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210710_000242.png" alt="img" /></p><p><a id="org8fbd907"></a></p><h2 id="改进结果-1">改进结果</h2><p>COCO数据集上的性能：可见YOLOv3的性能与SSD基本一致，但是比SSD快3倍，低于RetinaNet，但是RetinaNet的infer时间为YOLOv3的3.8倍，且0.5IOU下两者的性能比较接近。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210710_000303.png" alt="img" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2018-10-14-YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/2018_10_14_Yolov1_20210710_000328.png" alt="img" /></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AE%97%E6%B3%95%E7%AF%87/'>算法篇</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/paper-reading/" class="post-tag no-text-decoration" >Paper_Reading</a> <a href="/tags/cv/" class="post-tag no-text-decoration" >CV</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=YOLO论文系列解读 - Johney Zheng&url=https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=YOLO论文系列解读 - Johney Zheng&u=https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=YOLO论文系列解读 - Johney Zheng&url=https://www.johneyzheng.top//posts/YOLO%E8%AE%BA%E6%96%87%E7%B3%BB%E5%88%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E6%A0%88/">大模型推理技术栈</a><li><a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">模型部署技术概览</a><li><a href="/posts/FlashAttention%E7%B3%BB%E5%88%97%E4%BC%98%E5%8C%96/">FlashAttention系列优化</a><li><a href="/posts/Win10_Ubuntu_installation/">双系统安装(WIN10+Ubuntu16)</a><li><a href="/posts/nms_soft-nms/">nms and soft-nms</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">文章目录</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>接下来阅读</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/AVOD%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"><div class="card-body"> <span class="timeago small" > Oct 12, 2019 <i class="unloaded">2019-10-12T23:56:20+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>AVOD论文解读</h3><div class="text-muted small"><p> 目录 前言 论文基本信息 动机和思路 算法基本流程 具体实验分析 个人总结 前言 AVOD作为基于Camera信息和Lidar信息融合的3D detection算法，是目前为数不多的开源的SOTA算法之一，相关资料如下： KITTI榜单 AVOD论文 AVOD源码 AVOD源码解读 论文基本信息 一作信息： Jason Ku 多伦多大学...</p></div></div></a></div><div class="card"> <a href="/posts/Cont-Fuse%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"><div class="card-body"> <span class="timeago small" > Jan 3, 2020 <i class="unloaded">2020-01-03T23:56:20+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Cont-Fuse论文解读</h3><div class="text-muted small"><p> 目录 论文背景以及基本思想 算法基本流程 算法细节 Deep Continuous Fusion for Multi-Sensor 3D Object Detection 论文背景以及基本思想 出发点：作者认为Lidar数据与Image数据融合的难点在于怎么将稀疏连续的三维点云信息与语义丰富但是离散的图像数据融合，提出了基于连续卷积的point-wise Fusi...</p></div></div></a></div><div class="card"> <a href="/posts/F-ConvNet%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"><div class="card-body"> <span class="timeago small" > Jan 9, 2020 <i class="unloaded">2020-01-09T23:56:20+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>F-ConvNet论文解读</h3><div class="text-muted small"><p> 目录 论文背景以及基本思想 算法基本流程 算法细节 Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection 论文背景以及基本思想 论文出发点:本文基于F-PointNet的思想，认为F-PointNet非端到端，最...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Numpy%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/" class="btn btn-outline-primary" prompt="较早文章"><p>Numpy常用函数</p></a> <a href="/posts/Cont-Fuse%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="btn btn-outline-primary" prompt="较新文章"><p>Cont-Fuse论文解读</p></a></div><div id="comments"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ el: '#comments', app_id: 'IJm2s0GdkzhEOLwVfClrHeWs-gzGzoHsz', app_key: 'Y281bajarkkIGs8p4WmrTkNi', placeholder: '请在下面评论：', visitor: true }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">Johney Zheng</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，否则本网站上的博客文章均由作者根据知识共享许可协议 - 署名标示 4.0（CC BY 4.0）进行授权许可。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.johneyzheng.top/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
