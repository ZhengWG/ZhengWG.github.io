<!DOCTYPE html><html lang="zh-Hans" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="2D_Detection-经典检测框架介绍" /><meta property="og:locale" content="zh_Hans" /><meta name="description" content="前言 Two Stage算法 RCNN系列 Faster-RCNN总览 基于Faster-RCNN改进算法 One Stage算法 SSD YOLO YOLO V1 YOLO V2 YOLO V3 YOLO V4 &amp;&amp; YOLO V5 Anchor Free算法 FCOS CenterNet" /><meta property="og:description" content="前言 Two Stage算法 RCNN系列 Faster-RCNN总览 基于Faster-RCNN改进算法 One Stage算法 SSD YOLO YOLO V1 YOLO V2 YOLO V3 YOLO V4 &amp;&amp; YOLO V5 Anchor Free算法 FCOS CenterNet" /><link rel="canonical" href="https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/" /><meta property="og:url" content="https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/" /><meta property="og:site_name" content="Johney Zheng" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-29T15:47:39+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="2D_Detection-经典检测框架介绍" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"前言 Two Stage算法 RCNN系列 Faster-RCNN总览 基于Faster-RCNN改进算法 One Stage算法 SSD YOLO YOLO V1 YOLO V2 YOLO V3 YOLO V4 &amp;&amp; YOLO V5 Anchor Free算法 FCOS CenterNet","url":"https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/","headline":"2D_Detection-经典检测框架介绍","dateModified":"2021-08-29T22:29:09+08:00","datePublished":"2021-08-29T15:47:39+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"},"@context":"https://schema.org"}</script><title>2D_Detection-经典检测框架介绍 | Johney Zheng</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Johney Zheng"><meta name="application-name" content="Johney Zheng"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-200658716-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-200658716-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Johney Zheng</a></div><div class="site-subtitle font-italic">Johney Zheng的小站</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/ZhengWG" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['21625111','zju.edu.cn'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>2D_Detection-经典检测框架介绍</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>2D_Detection-经典检测框架介绍</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Johney Zheng </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Aug 29, 2021, 3:47 PM +0800" prep="on" > Aug 29, 2021 <i class="unloaded">2021-08-29T15:47:39+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Aug 29, 2021, 10:29 PM +0800" prefix="Updated " > Aug 29, 2021 <i class="unloaded">2021-08-29T22:29:09+08:00</i> </span> </span> <span><div id="mathjax"></div><script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ["$","$"]], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], processEscapes: true } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(); for (var i = 0; i < all.length; ++i) all[i].SourceElement().parentNode.className += ' has-jax'; }); </script> <script src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6794 words">37 min</span></div></div><div class="post-content"><ul><li><a href="#sec-1">前言</a><li><a href="#sec-2">Two Stage算法</a><ul><li><a href="#sec-2-1">RCNN系列</a><ul><li><a href="#sec-2-1-1">Faster-RCNN总览</a><li><a href="#sec-2-1-2">基于Faster-RCNN改进算法</a></ul></ul><li><a href="#sec-3">One Stage算法</a><ul><li><a href="#sec-3-1">SSD</a><li><a href="#sec-3-2">YOLO</a><ul><li><a href="#sec-3-2-1">YOLO V1</a><li><a href="#sec-3-2-2">YOLO V2</a><li><a href="#sec-3-2-3">YOLO V3</a><li><a href="#sec-3-2-4">YOLO V4 &amp;&amp; YOLO V5</a></ul></ul><li><a href="#sec-4">Anchor Free算法</a><ul><li><a href="#sec-4-1">FCOS</a><li><a href="#sec-4-2">CenterNet</a></ul></ul><h1 id="前言">前言<a id="sec-1"></a></h1><p>本文简述2D检测算法中经典的算法框架，该领域过去涌现了很多论文和框架，无法一一追踪。但是大部分新算法还是基于经典的算法框架上进行延生和拓展。 当前目标检测框架按照算法流程上看主要分为两个派别:One Stage和Two Stage.其中One Stage的代表算法有YOLO,SSD;Two Stage则以RCNN系列为主. Google基于Tensorflow实现的主流检测算法性能结果: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161308.png" alt="img" /></p><p>backbone对性能的影响: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161506.png" alt="img" /></p><p>物体尺寸的影响： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161555.png" alt="img" /></p><p>整体上看,精度上看Faster-RCNN的精度最高,且采用更好的backbone性能提升幅度也更大.速度最快的为SSD+MobileNet的组合,但是SSD的小物体检测性能较差. 后续也有一些基于 <code class="language-plaintext highlighter-rouge">Anchor Free</code> 的算法框架提出，这里也会略作介绍。</p><h1 id="two-stage算法">Two Stage算法<a id="sec-2"></a></h1><h2 id="rcnn系列">RCNN系列<a id="sec-2-1"></a></h2><p>RCNN系列经历了RCNN-&gt;Fast RCNN-&gt;Faster RCNN的变迁,最终的Faster RCNN可以认为是Two Stage的集大成者,这里也仅重点介绍下Faster RCNN的算法设计.</p><h3 id="faster-rcnn总览">Faster-RCNN总览<a id="sec-2-1-1"></a></h3><p>Faster-RCNN从功能模块来看,可以分为4个部分:</p><ul><li>特征提取网络(Backbone):输入图像通过Backbone得到特征图.<li>RPN模块:区域Proposal生成模块,其作用是生成较好的建议框,这里利用了强先验的Anchor.RPN包含了5个子模块:<ul><li>Anchor生成:RPN对feature map上每个点都对应生成9个Anchors,9个Anchors的大小宽高都不一致,几乎能够对应覆盖原图中的所有大小的物体.RPN的工作比那时从中筛选得到更好的Proposal位置.<li>RPN卷积网络:feature map上单个点上对应9个Anchors,可以通过卷积网络得到每个Anchor的预测得分和偏移<li>计算RPN loss:RPN阶段Anchor的标签分配只有正样本,负样本.RPN loss对每个Anchor的预测值包括两个部分:正负样本分类损失和预测偏移值损失.计算loss中,RPN默认随机选择256个Anchors进行损失的计算,其中最多不超过128个正样本,如果数量超过128,则进行随机选取.<li>生成Proposal:利用前置步骤得到的Anchor的的得分和偏移量,得到一组较好的Proposals<li>筛选Proposal得到最终RoI:训练过程中,现有Proposals数量还是过多(NMS去重后,按照score排序,默认2000),需要进一步筛选Proposal得到RoI计算loss(默认数量为256).infer过程中,不需要此模块,Proposal直接作为RoI(默认为300).训练过程中的筛选过程与计算RPN loss过程类似.其正负例的筛选标准:最大IoU大于0.5,则认为正样本;最大IoU大于0且小于0.5,则认为负样本.正负样本的总数为256,控制正负样本为1:3,即正样本的数目为64,负样本数目为192.</ul><li><p>RoI Pooling:RoI Pooling的作用是将大小不同的RoI区域转化为固定大小的特征输入.最早应用于SPPNet,在Fast RCNN中采用最近邻差值算法将池化过程进行了简化,而在随后的Mask RCNN上则通过RoI Align提高了算法的精度.RoI Align相较于RoI Pooling不再进行取整操作,得到的RoI区域直接为浮点数格式边界,即得到的区域和feature map点区域不match.对于不match的区域,通过取每个bin的四个点特征值的最大值作为该bin的特征值.四个点的特征值则通过周围feature map的特征值差值得到: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161727.png" alt="img" /></p><li>RCNN模块:经过RoI Pooling(RoI Align)后,得到了大小固定的特征,之后便可以通过全连接网络进行分类和回归预测量的计算.RCNN的loss计算和RPN的loss计算类似,只是分类loss为GT lable的loss,而不再是简单的正负例样本loss.</ul><p>Faster-RCNN的算法框架图如下: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161814.png" alt="img" /></p><h3 id="基于faster-rcnn改进算法">基于Faster-RCNN改进算法<a id="sec-2-1-2"></a></h3><p>Faster-RCNN算法作为Two-Stage网络,拥有较为优越的性能.但是仍然存在一些缺点:</p><ul><li>RCNN模块采用的是全连接网络,即所有RoI都会通过全连接得到,导致网络的计算量较大,速度较慢<li>后处理策略:NMS的后处理对于拥挤/遮挡场景不友好,容易造成漏检<li>正负样本的选取:当前正负样本采用的1:3的超参设计,但是该参数是否通用于其他场景,尚需讨论</ul><ol><li><p>R-FCN</p><p>Faster RCNN采用了两个全连接网络提取特征,占据了大部分的网络参数.但是如果直接去除全连接,检测性能则会急剧下降,主要在于基础卷积网络具有平移不变性质,但是对于位置信息不敏感. R-FCN通过全卷积网络得到位置敏感得分图(position-sensitive score maps)实现了对位置的敏感性.R-FCN的算法框架图如下: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161844.png" alt="img" /></p><p>R-FCN的算法流程如下:</p><ul><li>网络Backbone提取图像特征<li>通过全卷积网络基于特征图生成位置敏感图的score bank,假设设计目标的相对位置数量为k^2,则分数图数目为k^2*(C+1),C为类别数目<li>采用RPN网络得到RoI,然后将其分为k^2区域,并将子区域作为分数图<li>当k^2子区域都具备某类的目标匹配值时,则对该子区域求取平均值,同理得到每类的score,最终vote得到C+1维度的特征<li>对剩下的C+1维度进行softmax回归,进行分类</ul><p>R-FCN的优势在于保持精度一致的情况下,速度明显优于Faster-RCNN.</p><li><p>Cascade RCNN</p><p>Cascade RCNN较为深入地探讨了IoU阈值对检测器性能的影响.作者提出了原始Faster RCNN在RPN阶段存在mismatch问题:</p><ul><li>training阶段,由于gt是已知的,所有可以取iou大于0.5的作为正样本<li>infer阶段,gt未知,只能将所有的proposal作为正样本,通过score判断正负样本</ul><p>所以,training阶段和infer阶段,bbox回归的输入分布是不一样的,该missmatch现象在threshold=0.5的时候问题不会太大.但是当threshold提高的时候,一方面missmatch现象会更严重,同时也会由于训练过程中正样本Proposal变少,导致模型过拟合. 因此提高检测位置精度,直接提高IoU阈值的方式是不合理的.作者的改进的思路主要基于两点:</p><ul><li>当Proposal自身的IoU阈值和训练器用的阈值较为接近的时候,训练器的性能更好<li>经过回归后候选框与GT的IoU会有所提升.</ul><p>Cascade RCNN的算法设计如下:</p><ul><li>采用multi-stage的算法框架,每个检测stage采用不同的IoU阈值,且IoU阈值逐步变高<li>前一个回归网络的输出边框作为下一个检测器的输入继续回归,共迭代三次</ul><p>整体算法框架如下: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_161958.png" alt="img" /></p></ol><h1 id="one-stage算法">One Stage算法<a id="sec-3"></a></h1><h2 id="ssd">SSD<a id="sec-3-1"></a></h2><p>SSD算法框架: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163237.png" alt="img" /></p><p>SSD模型主要分为以下几个部分:</p><ul><li><code class="language-plaintext highlighter-rouge">Multi-scale feature maps for detection</code> : backbone采用基础网络结构(VGG),本文修改了backbone的最后几层结构，同时引入了额外的卷积层用于抽取不同scale尺度的feature<li><code class="language-plaintext highlighter-rouge">Convolutional predictions for detection</code> : 对于每个feature map(假设size为mxnxp)，通过3x3的卷积核生成预测value，包括预测位置的相对值(相对feature map像素对应的原图位置)以及confidence等<li><code class="language-plaintext highlighter-rouge">Default boxes and aspect ratios</code> : 每层的feature map设置有默认大小和长宽比的default boxes，feature map上的每个位置会基于deafault box作相对位置和大小的估计以及类别score的估计。对于size为mxn的feature map，其最终的预测结果为(c+4)xkxmxn，其中c为类别数目，4为位置偏移值和大小偏移值，k为default box数目(不同scale的feature map的k值有所不同，如conv10_2和conv11_2采用k=4,余下为6)。</ul><p>算法细节:</p><ul><li><code class="language-plaintext highlighter-rouge">Base network</code> : 在VGG的基础上，进行了一系列的改动:fc6, fc7替换为卷积层，pool5替换2x2-s2为3x3-s1，同时采用空洞卷积，去除所有drop-out层和fc8层。<li><code class="language-plaintext highlighter-rouge">Matching strategy</code> : 区别于<a href="https://arxiv.org/pdf/1312.2249.pdf">MultiBox</a>，SSD对所有与GT的IOU阈值大于0.5的default box作为positive，即default box与GT是多对一的关系，而不是MultiBox的取最大IOU的一对一的关系<li><p><code class="language-plaintext highlighter-rouge">Loss</code> 设计：Loss主要分为两个部分:localization loss和confidence loss: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163309.png" alt="img" /></p><p>其中，N为匹配上的box数目，如N=0，则loss=0。localization loss采用的是 <code class="language-plaintext highlighter-rouge">L1 loss，localization loss</code> 包括(cx,cy,w,h)，即中心位置和宽，长四部分loss，confidence loss采用的是 <code class="language-plaintext highlighter-rouge">softmax loss</code> 。</p><li><p><code class="language-plaintext highlighter-rouge">Choosing scales and aspect ratios for detection</code> :不同尺度的ferature map拥有不同的感受野，所有应当设计不同大小的default box( <strong>影响性能比较关键的点</strong> )。SSD采用的default box大小设计如下: <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163337.png" alt="img" /></p><p>其中，s_min为0.2，s_max为0.9,长宽比设计为 $a_r={1,2,3,1/2,1/3}$</p><li><code class="language-plaintext highlighter-rouge">Hard negative mining</code> :策略为按照confidence loss进行排序，保持正负样本为1:3<li><code class="language-plaintext highlighter-rouge">Data augmentation</code> :采用的ramdom crop策略( <strong>非常重要</strong> ):<ul><li>输入原始大小的输入图片<li>在输入图片上进行Sampel得到patch，满足与objects的最小jaccard overlap为0.1,0.3,0.5,0.7 or 0.9<li>随机Sample得到patch: sample得到的patch大小为[0.1,1],长宽比为1/2,2，保留中心点在sampled patch的box。得到的patch会被resize到固定size，并且采用horizontally flip(probability 0.5),photo-metrix distortions等增强。</ul></ul><p>SSD算法作为One-Stage网络仍然存在一系列的算法限制:</p><ul><li>小物体检测性能较差<li>PriorBox大小和宽度依赖于人工设置,需要一定调试(关于Anchor的设计的相关算法其实有很多，部分经典论文：<a href="https://arxiv.org/pdf/1912.02424.pdf">ATSS</a>,<a href="https://arxiv.org/pdf/1903.00621.pdf">FSAF</a>;个人有时间也会简单整理下)</ul><h2 id="yolo">YOLO<a id="sec-3-2"></a></h2><p>YOLO系列截止2021/2的发展:v1-&gt;v2-&gt;v3-&gt;v4-&gt;v5,其中v4,v5非YOLO原作,后期更多是算法的trick的叠加应用,这里重点介绍v1/v2/v3版本,v4/v5(updated on 8/29/2021)</p><h3 id="yolo-v1">YOLO V1<a id="sec-3-2-1"></a></h3><p>YOLOv1：</p><ul><li><p><code class="language-plaintext highlighter-rouge">Unified Detection</code> :</p><p>YOLO将图片分为S*S的栅格，每个栅格对中心落在栅格内部的物体负责。每个栅格会预测B个bounding box，confidence信息和位置信息:x,y,w,h。</p><p>其中confidence信息表示为条件概率的形式: $confidence=Pr(Object)*IOU$ ，即如果bounding box预测无物体，则 $Pr(object)=0$ ，则confidence为0，如果预测有物体，则confidence为预测box和gt的IOU。</p><p>其中,x,y表示预测的bounding box的中心与栅格边界的相对位置，w，h表示为bounding box的width,height相对于整幅图像的比例。</p><p>另外每个grid cell还会预测c类的conditional confidence: $Pr({Class_i}|Object)$ ,infer过程中通过 $Pr({Class_i}|Object)*Pr(Object)*IOU$ 来作为bounding box的类预测confidence。</p><p>注意这里的class confidence只针对c个类，而bounding box的confidence则针对每个box：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163437.png" alt="img" /></p><li><p><code class="language-plaintext highlighter-rouge">Network Design</code>:</p><p>YOLO的网络借鉴了GoogleNet，通过 $1*1$ 和 $3*3$ 的卷积层替代了Inception结构。标准的YOLO网络有24层卷积层，后接两层全连接层，而Fast Yolo则只有9层卷积层。YOLO的输出tensor大小为 $7*7*30$ 。与prediction的size相匹配。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163456.png" alt="img" /></p><li><code class="language-plaintext highlighter-rouge">Training</code>: 先在ImageNet 1000-class分类任务上的进行网络的Pretrain，使用上述网络中的前20层卷积层， 后接一个average-pooling层和一个全连接层。将Pretrain得到的前20层全连接层作为Detection网络的前置网络，并加入后续的4层卷积层以及两个全连接层，最后层预测得到class probility以及bounding box coordinates。其中w,h,x,y需要归一化到0-1，以保证w,h小于图片尺寸，且位置在特定的grid cell边界范围内。<li><code class="language-plaintext highlighter-rouge">Loss设计</code> Loss采用 <code class="language-plaintext highlighter-rouge">sum-squared error loss</code> ，但是对不同类的loss采用了不同的权重设计：<ul><li>coordinates error需要有更高的权重<li>no-object的栅格数目比重很大，其所占的loss很大，需要降低no-object的loss权重，降低对网络的贡献率<li>大物体小物体的误差容忍率应该是不一致的，小物体偏移对IOU影响更大，作者采用原本height和width的平方根代替原始值<li>训练过程中，可能存在多个box预测同一个物体，则只取IOU最大的predictor作最终的预测，最终的loss设计如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_162304.png" alt="img" /></ul></ul><h3 id="yolo-v2">YOLO V2<a id="sec-3-2-2"></a></h3><p>作者的改进方案如下：</p><ul><li><code class="language-plaintext highlighter-rouge">BN层</code> ：在对所有卷积层加入BN层后，提升了2%的mAP<li><code class="language-plaintext highlighter-rouge">High Resolution Classifier</code>: YOLOv1训练分类网络时采用的input size为224X224，但是detection网络采用的输入为448X448，意味着detection网络训练的过程中需要重新学习更大的输入尺寸。YOLOv2先将分类网络在448X448的输入下进行finetune,然后进行detection网络的finetune，提升了4%的mAP<li><code class="language-plaintext highlighter-rouge">Convolutional With Anchor Boxes</code>: 采用anchor box（k-means聚类得到）的方式进行bounding box的预测。首先移除了YOLOv1的全连接层以卷积层替代，移除最后的pooling层，增大输出的size，输入的图片size由448X448-&gt;416X416,目的是为了保持输出为13X13的奇数，以保留图片正中间的grid cell位置（作者认为这对图片中间的大物体预测有帮助）。另外box的class的预测仍然沿用之前的YOLOV1的策略，位置预测直接预测中心点和大小，其中中心点预测采用sigmoid函数限制预测bbox在对应的anchor内。该方案的收益是提高了7%的recall，但是accuracy下降:69.5mAP@81%recall-&gt;69.2mAP@88%recall。<li><p><code class="language-plaintext highlighter-rouge">Dimension Clusters</code>: 采用K-means进行box的聚类，相同情况下，能够比hand picked得到的box有更高的Avg IOU。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163622.png" alt="img" /></p><li><p><code class="language-plaintext highlighter-rouge">Direct location prediction</code>: 默认的bounding box的center point的预测方案：</p>\[x = (t_x * w_a) - x_a\] \[y = (t_y * h_a) - y_a\]<p>该方案的问题是偏移的最大值与anchor box的大小有关，没有其他约束，导致center点的位置有可能到图像中的任意位置，导致模型训练不稳定。所以作者沿用了原先YOLO的预测位置的方式。另外作者通过Dimension Clusters和Direct location prediction的方式，提高了anchor-version 5% mAP,示意图如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163725.png" alt="img" /></p><li><code class="language-plaintext highlighter-rouge">Fine-Grained Features</code>: 通过pass-through（ <code class="language-plaintext highlighter-rouge">stacking adjacent features into diffrent channels</code> ）的方式将前置的feature层（26X26）与原先的feature map进行concat，融合多尺度性能，能够提高1% mAP。<li><code class="language-plaintext highlighter-rouge">Multi-Scale Training</code>: 为了让YOLOv2对input_size有更强的鲁棒性，模型训练过程中每10个batches会选择一个新的image size输入:{320,352,…,608}。<li><code class="language-plaintext highlighter-rouge">Faster</code>: 设计了Darknet-19:5.58 billion operation,72.9% top1 accuracy, 91.2% top5 accuracy on ImageNet.</ul><h3 id="yolo-v3">YOLO V3<a id="sec-3-2-3"></a></h3><p>YOLOv3的改进更多地借鉴了诸如SSD，Faster-rcnn，FPN的优点，主要改进如下：</p><ul><li>Predictions Across Scales：参照FPN的设计，对原本13X13的输出feature进行上采样，分别得到26X26和52X52尺寸的feature，并与浅层的featue进行concat，最后通过卷积层进行预测。K-means采用的cluster数目为9。<li>Feature Extractor：加深了backbone的层数，并且引入Resnet的残差结构，实现了低运算量下较好的性能： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_163843.png" alt="img" /></ul><h3 id="yolo-v4--yolo-v5">YOLO V4 &amp;&amp; YOLO V5<a id="sec-3-2-4"></a></h3><p>YOLO V5/V4在YOLO V3基础上整合了SOTA结构，进一步提升了基于YOLO框架的One-Stage算法的性能。 参考： <a href="https://zhuanlan.zhihu.com/p/161083602">知乎文章</a> 进行简单总结：</p><ul><li><p>Data Augmentation</p><p>YOLOV4：</p><ul><li>Random Erase:随机值/训练集平均像素替换图像区域<li>Cutout：剪切方块进行mask<li>Hide and Seek：以SXS方格进行图片分割，然后随机进行隐藏<li>Gide Mask：采用Grid Mask的方式进行图片的Mask操作<li>MixUp：图像/标签的混合叠加<li>Cutmix：剪切图片并进行粘贴<li>Mosaic data augmentation：四张图像按一定比例进行组合<li>自对抗训练（SAT）：通过反向传播改变图片信息进行增强（这里反转不改变网络参数）<li>Lable Smoothing：通过对class label进行编码，将原始的one-hot-label转换为soft-label。</ul><p>YOLOV5：</p><ul><li>缩放<li>色彩空间调整<li>Mosaic data augmentation</ul><li><p>Backbone</p><p>YOLOV4/YOLOV5都采用了CSPDarknet作为BackBone:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_164142.png" alt="img" /></p><p>其核心在于基于Densnet的思想，通过dense block不断提取深度特征，并且通过复制前置层的操作进行网络特征重用，通过跨阶段特征融合和截断梯度流的方式来增强不同层间学习特征的可变性。</p><li><p>Neck</p><p>YOLOV4/YOLOV5都采用了PANET作为Neck来聚合特征，其结构如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_164224.png" alt="img" /></p><p>其核心在于添加了自下而上路径的FPN结构，改善来低层feature的传播，同时采用了Adaptive feature pooling的方式实现各个feature map对应区域的特征聚合。</p><li><p>Det-Head</p><p>YOLOV4/YOLOV5采用了和YOLOV3一致的Det-Head。</p><li>Loss YOLO系列的Loss包括：objectness loss/class probility loss/bounding bbox regression loss 其中YOLOV5采用cross-entropy/Focal loss计算分类损失，GIoU Loss作为regression loss;YOLOV4采用CIoU loss作为regression loss。<li>Comparation 根据当前资料上看，两者性能十分接近。其中，YOLOV4数字上性能更好，且可定制化较高；YOLOv5的训练速度很快，且提供了一系列轻量级网络，方便部署。</ul><h1 id="anchor-free算法">Anchor Free算法<a id="sec-4"></a></h1><p>其实将Anchor Free算法和Two Stage算法并列不是很合理，因为其本质还是可以归为One Stage算法，只是在Proposal的获取层面摆脱了Anchor的束缚。Anchor Free算法的出发点在于：Anchor Based本身上的一些缺陷：</p><ul><li>算法精度依赖于Anchor的设计，不同场景不同对象的大小尺寸分布对Anchor设计存在适应性，极端场景下不同Anchor设计存在较大的精度差距<li>Anchor实际上存在一定冗余计算，比如：单个bbox往往对应多个anchor，但是能match的target往往只有一个，意味其他都是多余的match的框，一定程度上也会带来训练过程中的正例竞争<li>NMS引入，多个anchor的设计必然会带来很多正例的match框，通常的解法是通过nms来解决，但是这种方案一方面nms有时候较为耗时，同时部分场景下，nms可能会滤掉一些正例（拥挤场景）</ul><p>所以，Anchor Free算法通常网络设计更简单高效，易于部署，但是性能上限不高。本文主要介绍两种业界应用比较多的经典Anchor Free算法：FCOS和CenterNet；关于Anchor Free算法和Anchor Based算法的性能和精度差距，<a href="https://arxiv.org/pdf/1912.02424.pdf">ATSS</a>这篇论文阐述得比较详细，大家有兴趣可以参考下。</p><h2 id="fcos">FCOS<a id="sec-4-1"></a></h2><p><a href="https://arxiv.org/pdf/1904.01355.pdf">FCOS</a>发表于2019 ICCV，代码开源于 <a href="https://github.com/tianzhi0549/FCOS">FCOS_Github</a>，类似于 <code class="language-plaintext highlighter-rouge">mmdetection</code> 都集成了相关的算法。</p><p>FCOS刚发布的时候精度超越了前作的所有基于Anchor Free的算法（笔者验证了其与设计过的Anchor based算法还是有一定差距，但是差距不大）。其算法优势是避免了Anchor的超参设计，网络速度更快，同时其一些算法设计上也带来了一些思考（ <code class="language-plaintext highlighter-rouge">Centerness</code> ）。</p><p>Anchor Free算法的问题是：失去了先验的Anchor设计，如何合理地分配正负例？其实YoloV1算法框架便是没有Anchor设计的，其采用的正例的分配的方案是：如果Target的中心点落在对应的方格内（Yolov1将feature map分为7*7的格子），则认为该正例归属于该方格，应由该方格负责预测。但是该方案的问题在于：一个方格只能预测一个正例，对拥挤场景性能差；同时该方案无法适应多feature map（如 <code class="language-plaintext highlighter-rouge">FPN</code> ）场景，在小物体上的性能表现差。</p><p>FCOS的核心思路是：不同大小的Feature map负责不同大小的物体的预测，训练时通过GT框的长宽进行分配；同时引入 <code class="language-plaintext highlighter-rouge">Centerness</code> 来提高匹配框的质量。 FCOS的算法框架如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_142400.png" alt="img" /></p><p>算法细节：</p><ul><li><code class="language-plaintext highlighter-rouge">Backbone</code> ：采用了RetinaNet的Backbone，特点是采用了FPN设计输出不同大小的5层feature map：P3～P7，各层之间通过stride为2的卷积产生，同时进行上下层的特征融合。<li><code class="language-plaintext highlighter-rouge">Anchor分配策略</code> ：<ul><li><p>目标定义：FCOS的预测策略是中心点（x,y）和偏移向量（l,t,r,b）来进行对象的预测： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_143312.png" alt="img" /></p><li>同层分配策略：按照目标中心点(x,y)位于哪个box则由该box进行预测，对于多个目标取面积最小的。<li>不同层的分配策略：设计不同层负责的目标范围，如P3～P7对应的尺寸范围 $(m_{i-1}, m_{i})$ 为：(0, 64),(64, 128),(128, 256),(256, 512),(512,+∞)。假设目标GT为（x<em>,y</em>）(l<em>,t</em>,r<em>,b</em>)，则根据如下公式进行分配： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_144012.png" alt="img" /></ul><li><code class="language-plaintext highlighter-rouge">Head设计</code> ：不同feature map采用共享权重的Head，Head的结构为 <code class="language-plaintext highlighter-rouge">Multi Convs + Classification/Regression/Centerness</code><ul><li><p>Clafication + Regression: 分类任务采用C个二分类实现，loss采用 <code class="language-plaintext highlighter-rouge">focal loss</code> ，回归任务主要预测4个偏移向量: l/t/r/b，loss采用的是 <code class="language-plaintext highlighter-rouge">IoU loss</code> ： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_144907.png" alt="img" /></p><li><p>Centerness: Centerness的出发点在于找到目标对应的最靠近中心点（质量最好）的方格进行预测回归。Centerness的定义如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_145250.png" alt="img" /></p><p>训练过程中可以采用BCE损失：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_145504.png" alt="img" /></p><li><p>Loss合并： 最终的Loss包含如下三部分： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_145702.png" alt="img" /></p></ul></ul><p>算法总结：</p><ul><li>优势<ul><li>全卷积的网络结构，整体结构非常简单高效；避免来Anchor参数设计，精度也比较可观<li>Anchor多层的分配策略解决了Target冲突的问题<li>Centerness的引入类似IoU branch，其带来的思考是：引入box预测质量的评估是有效的。</ul><li>缺陷/可提升点<ul><li>本质还是基于Anchor的目标检测方案（可以认为是单Anchor的变种），算法层面没有根本上的创新， ATSS其实更近一步思考了该类问题<li>在同尺度的密集场景存在正样本预测上的冲突（FCOS没有解决同层上的正样本的冲突问题）<li>Loss层面：Centerness可以通过IoU替换（效果更好？）；另外IoU loss也可以通过GIoU Loss进行替换：<a href="https://arxiv.org/pdf/2006.04388.pdf">Generalize Focal Loss</a></ul></ul><h2 id="centernet">CenterNet<a id="sec-4-2"></a></h2><p>CenterNet其实有两篇：<a href="https://arxiv.org/pdf/1904.08189.pdf">CenterNet: Keypoint Triplets for Object Detection</a> 和 <a href="https://arxiv.org/pdf/1904.07850.pdf">Objects as Points</a></p><p>其中第一篇的思路来源于Keypoint的预测，类似的前作有<a href="https://arxiv.org/pdf/1808.01244.pdf">CornerNet</a>。其核心思路是通过两个branch进行物体中心点和左上+右下脚点的预测从而得到最终的检测框。</p><p>本文主要介绍第二篇即：Objects as Points，原因主要是该方法结构非常简单高效，拓展性很强，在其基础可拓展：2D检测/3D检测/人体姿态估计/Tracking算法（<a href="https://arxiv.org/pdf/2004.01177.pdf">Tracking Objects as Points</a>），且本文一定程度上去掉NMS操作（采用来max_pool的方式），有很多启发性的亮点。</p><p>这里简述下其在2D Detection这个任务上的算法设计（其他Task其实很类似）： CenterNet的核心思路是通过基于预测Key-Points的方式来得到检测框。与类似CornerNet的方式不同，Cornet的方案是通过预测中心点/框边缘点，然后再通过 <code class="language-plaintext highlighter-rouge">grouping</code> 的方式来进行点的关联，这样一方面带来额外的计算量，同时也带来了误差；CenterNet的方法比较简单粗暴：直接基于预测的到的中心点进行中心点偏差和检测框的宽高的预测,预测效果如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_153846.png" alt="img" /></p><p>CenterNet算法细节：</p><ul><li><p><code class="language-plaintext highlighter-rouge">BackBone</code> : CenterNet预测得到物体中心点是基于Heatmap得到的，所以其网络结构与Keypoints/分割网络比较接近，为encoder-decoder类型的网络；论文采用了三种BackBone：Resnet-18 with up-convolutional layers/DLA-34/Hourglass-104，精度依次变高，速度依次变慢，另外其采用较大的feature map（下采样因子为4）,原因是其基于单feature map进行预测，feature map过小，无法预测小物体： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_154523.png" alt="img" /></p><li><p><code class="language-plaintext highlighter-rouge">正负例分配</code> ： CenterNet没有Anchor的概念，只有heatmap的heat点来预测正例。所以训练过程中需要将检测框的GT值进行转化，同样生成heatmap的GT图，可采用的高斯滤波的方式： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_155335.png" alt="img" /></p><li><p><code class="language-plaintext highlighter-rouge">Loss设计</code> ： CenterNet的预测值包括三个部分：中心点/中心点偏置/检测框长宽；预测中心点的偏置的原因是heatmap是下采样得到的，中心点存在误差。值得注意的是在计算中心点损失的过程中，会遇到正负例样本不平衡的问题，SSD等Anchor Based的方案通过正负例的loss比例或者Focal loss的方式来缓解正负例样本不均衡的问题。CenterNet也采用了类似的方案：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_155713.png" alt="img" /></p><p>对中心点偏置和长宽的loss都采用了L1 Loss：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_155859.png" alt="img" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_155917.png" alt="img" /></p><p>最后合并后的loss为：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_155956.png" alt="img" /></p><li><p><code class="language-plaintext highlighter-rouge">后处理</code> ： 后处理指的是预测阶段，CenterNet没有采用NMS的方案，而是通过一个3X3的max pooling得到100个heat点（排序得到），最后再通过类别score（Heat点概率）阈值得到最后的中心点，并结合中心点的偏置和长宽得到最终检测框： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2021-08-29-2D_Detection-%25E7%25BB%258F%25E5%2585%25B8%25E6%25A3%2580%25E6%25B5%258B%25E6%25A1%2586%25E6%259E%25B6%25E4%25BB%258B%25E7%25BB%258D/2d_detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6_20210829_160342.png" alt="img" /></p></ul><p>算法总结：</p><ul><li>优势<ul><li>结构非常简单直白，但是能work；且这种简单的结构具备很强的适配性，可应用于各项任务<li>几乎是全卷积/池化的操作，网络结构很高效，适合部署</ul><li>缺陷<ul><li>仍然受限于One Stage网络的缺陷：无法解决重叠的问题；另外由于其没有多层feature map，该现象会更明显。值得注意的是：在生成GT Heatmap的过程中，对于重叠的高斯分布点，算法采用了直接替代的方式（取重叠范围内更大的高斯点），导致其算法上限不高</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AE%97%E6%B3%95%E7%AF%87/'>算法篇</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/cv/" class="post-tag no-text-decoration" >CV</a> <a href="/tags/%E7%BB%BC%E8%BF%B0/" class="post-tag no-text-decoration" >综述</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=2D_Detection-经典检测框架介绍 - Johney Zheng&url=https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=2D_Detection-经典检测框架介绍 - Johney Zheng&u=https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=2D_Detection-经典检测框架介绍 - Johney Zheng&url=https://www.johneyzheng.top//posts/2D_Detection-%E7%BB%8F%E5%85%B8%E6%A3%80%E6%B5%8B%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E6%A0%88/">大模型推理技术栈</a><li><a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">模型部署技术概览</a><li><a href="/posts/FlashAttention%E7%B3%BB%E5%88%97%E4%BC%98%E5%8C%96/">FlashAttention系列优化</a><li><a href="/posts/Win10_Ubuntu_installation/">双系统安装(WIN10+Ubuntu16)</a><li><a href="/posts/nms_soft-nms/">nms and soft-nms</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">文章目录</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>接下来阅读</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Tracking%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"><div class="card-body"> <span class="timeago small" > May 31, 2020 <i class="unloaded">2020-05-31T15:41:56+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tracking算法综述</h3><div class="text-muted small"><p> 目标跟踪问题定义 目标跟踪算法简述 生成式模型 判别式模型 深度学习方法 目标跟踪问题定义 目标跟踪分为密集跟踪(a series detections)和稀疏跟踪(estimation+common sense)。前者本质上对每一帧进行检测，需要更大的计算量，实时性差。所以，一般目标跟踪问题都是指的稀疏跟踪：对检...</p></div></div></a></div><div class="card"> <a href="/posts/3D%E5%8D%95%E7%9B%AE(mono_3D)%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"><div class="card-body"> <span class="timeago small" > Jan 22, 2021 <i class="unloaded">2021-01-22T21:08:39+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>3D单目(mono 3D)目标检测算法综述</h3><div class="text-muted small"><p> 前言 算法调研(相对完善) 2D升3D问题 表达形式(Representation transformation): BEV, Pseudo-Lidar 关键点&amp;&amp;形状 通过2D/3D约束进行距离估计 直接生成3D候选区域(proposal) 关键总结 前言 翻译自:Monocu...</p></div></div></a></div><div class="card"> <a href="/posts/2D%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0(2D_Detection)/"><div class="card-body"> <span class="timeago small" > Jul 25, 2021 <i class="unloaded">2021-07-25T22:25:12+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>2D检测算法综述(2D_Detection)</h3><div class="text-muted small"><p> 前言 公开数据集 经典论文 基本结构 Backbone 超参 Loss 经典检测经典框架 网络加速 通用检测Tricks 部分论文解读Links 前言 update on 4/10/2021: 终于完成了2D_Detection系列，算是对之前自己在这个领域工作的一点回顾，之后自己会不定期更新一些深度学习模型部署的文章以及论文解读的文章 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95(Python)-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97&&%E6%9C%80%E5%A4%A7(%E5%B0%8F)%E5%A0%86/" class="btn btn-outline-primary" prompt="较早文章"><p>数据结构与算法(Python)-优先队列&&最大(小)堆</p></a> <a href="/posts/2D_Detection-%E6%A8%A1%E5%9E%8B%E5%8A%A0%E9%80%9F(%E7%BD%91%E7%BB%9C%E7%AF%87)/" class="btn btn-outline-primary" prompt="较新文章"><p>2D_Detection-模型加速(网络篇)</p></a></div><div id="comments"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ el: '#comments', app_id: 'IJm2s0GdkzhEOLwVfClrHeWs-gzGzoHsz', app_key: 'Y281bajarkkIGs8p4WmrTkNi', placeholder: '请在下面评论：', visitor: true }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">Johney Zheng</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，否则本网站上的博客文章均由作者根据知识共享许可协议 - 署名标示 4.0（CC BY 4.0）进行授权许可。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.johneyzheng.top/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
