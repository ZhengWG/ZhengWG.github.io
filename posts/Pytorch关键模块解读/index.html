<!DOCTYPE html><html lang="zh-Hans" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Pytorch关键模块解读" /><meta property="og:locale" content="zh_Hans" /><meta name="description" content="目录" /><meta property="og:description" content="目录" /><link rel="canonical" href="https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/" /><meta property="og:url" content="https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/" /><meta property="og:site_name" content="Johney Zheng" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-07-04T21:42:06+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Pytorch关键模块解读" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"目录","url":"https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/","headline":"Pytorch关键模块解读","dateModified":"2022-07-17T17:38:05+08:00","datePublished":"2021-07-04T21:42:06+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/"},"@context":"https://schema.org"}</script><title>Pytorch关键模块解读 | Johney Zheng</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Johney Zheng"><meta name="application-name" content="Johney Zheng"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-200658716-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-200658716-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Johney Zheng</a></div><div class="site-subtitle font-italic">Johney Zheng的小站</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/ZhengWG" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['21625111','zju.edu.cn'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Pytorch关键模块解读</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Pytorch关键模块解读</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Johney Zheng </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Jul 4, 2021, 9:42 PM +0800" prep="on" > Jul 4, 2021 <i class="unloaded">2021-07-04T21:42:06+08:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Jul 17, 2022, 5:38 PM +0800" prefix="Updated " > Jul 17, 2022 <i class="unloaded">2022-07-17T17:38:05+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5644 words">31 min</span></div></div><div class="post-content"><h1 id="目录">目录</h1><ol><li><a href="#org309e32d">前言</a><li><a href="#orga1164fa">torch.autograd: 梯度计算</a><li><a href="#org15a7aa6">BN &amp; SyncBN: BN与多卡同步BN</a><li><a href="#org3d3b613">torch.utils.data: 解析数据处理全流程</a><li><a href="#org0922dff">nn.Module: 核心网络模块接口</a><li><a href="#org69d7f8e">DP &amp; DDP: 模型并行和分布式训练</a><li><a href="#org62c086e">torch.optim: 优化算法接口</a><li><a href="#orga911203">torch.cuda.amp: 自动混合精度</a><li><a href="#orge28e334">cpp_extension: C++/CUDA算子实现和调用全流程</a></ol><p><a id="org309e32d"></a></p><h1 id="前言">前言</h1><p>主要参考：<a href="https://zhuanlan.zhihu.com/p/328674159">MMLab知乎源码解读</a> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_222216.png" alt="img" /></p><p><a id="orga1164fa"></a></p><h1 id="torchautograd-梯度计算">torch.autograd: 梯度计算</h1><p>梯度计算主要涉及以下几个模块：</p><ul><li><code class="language-plaintext highlighter-rouge">torch.autograd.function</code> (函数的反向传播) <code class="language-plaintext highlighter-rouge">Pytorch</code> 中的模块<code class="language-plaintext highlighter-rouge">nn.Module</code> 通常是包裹了<code class="language-plaintext highlighter-rouge">autograd function</code> ，以其作为真正实现的部分，如<code class="language-plaintext highlighter-rouge">nn.ReLU</code> 实际使用的是：<code class="language-plaintext highlighter-rouge">torch.nn.functional.relu(F.relu)</code><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>    <span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

    <span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
        <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s">'inplace'</span><span class="p">]</span>
        <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span>

        <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">ReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">inplace</span> <span class="o">=</span> <span class="n">inplace</span>

        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
            <span class="c1"># F.relu实际包裹的函数类型为builtin_function_or_method
</span>            <span class="c1"># 其内部定义了forward，backward函数，描述了梯度前向/反向传播的过程
</span>            <span class="c1"># 通常使用C++实现（如ATen）
</span>            <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">inplace</span><span class="p">)</span>
</pre></table></code></div></div><p>基于<code class="language-plaintext highlighter-rouge">torch.autograd.function</code> 中定义的<code class="language-plaintext highlighter-rouge">Function</code> 类作为基类可以实现自定义的<code class="language-plaintext highlighter-rouge">autograd function</code> 函数：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre>    <span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span> <span class="c1"># 此层计算e^x
</span>
        <span class="o">@</span><span class="nb">staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span> <span class="c1"># 模型前向
</span>            <span class="n">result</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">exp</span><span class="p">()</span>
            <span class="n">ctx</span><span class="p">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="c1"># 保存所需内容，以备backward时使用，所需的结果会被保存在saved_tensors元组中；此处仅能保存tensor类型变量，若其余类型变量（Int等），可直接赋予ctx作为成员变量，也可以达到保存效果
</span>
        <span class="o">@</span><span class="nb">staticmethod</span>
        <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span> <span class="c1"># 模型梯度反传
</span>            <span class="n">result</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">save_tensors</span> <span class="c1"># 取出forward中保存的result
</span>            <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">result</span>


        <span class="c1"># 使用样例
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># 注意设置tensor的requires_grad为True
</span>        <span class="n">ret</span> <span class="o">=</span> <span class="n">Exp</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 使用apply方法调用自定义autograd function
</span>        <span class="k">print</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="c1"># (tensor[2.7183], grad_fn=&lt;ExpBackward&gt;)
</span>        <span class="n">ret</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 反传梯度
</span>        <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># tensor([2.7183])
</span></pre></table></code></div></div><li><code class="language-plaintext highlighter-rouge">torch.autograd.functional</code> (计算图的反向传播) Tensor类的backward方法实际调用的是<code class="language-plaintext highlighter-rouge">torch.autograd.backward</code> 接口。pytorch实现中，autograd会记录生成前variable的所有操作，并建立一个有向无环图(DAG)。反向传播的过程中，autograd会沿着图从当前变量（根结点F）溯源，可以通过链式求导法则计算所有叶子节点的梯度。以下简单实现在计算图上进行autograd：<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>    <span class="c1"># 样例仅适用于每个op只产生一个输出的情况，且效率很低
</span>    <span class="k">def</span> <span class="nf">autograd</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="n">auto_grad</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">queue</span> <span class="o">=</span> <span class="p">[[</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">gradient</span><span class="p">]]</span>
        <span class="k">while</span> <span class="n">queue</span> <span class="o">!=</span> <span class="p">[]:</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">queue</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="n">gradients</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">functions</span> <span class="o">=</span> <span class="p">[[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">next_functions</span><span class="p">]]</span>
</pre></table></code></div></div><li><code class="language-plaintext highlighter-rouge">torch.autograd.gradcheck</code> (数值梯度检查)<li><code class="language-plaintext highlighter-rouge">torch.autograd.anomaly_mode</code> (在自动求导时检测错误产生路径)<li><code class="language-plaintext highlighter-rouge">torch.autograd.grad_mode</code> (设置是否需要梯度)<li><code class="language-plaintext highlighter-rouge">model.eval和torch.no_grad()</code><li><code class="language-plaintext highlighter-rouge">torch.autograd.profiler</code> (提供function级别的统计信息)</ul><p><a id="org15a7aa6"></a></p><h1 id="bn--syncbn-bn与多卡同步bn">BN &amp; SyncBN: BN与多卡同步BN</h1><p>简单谈下BN的优势：</p><ul><li>防止过拟合：结合mini-batch训练，能够避免对单个样本的过拟合<li>加快收敛：原始是BN的归一化效果<li>防止梯度弥散/爆炸：归一化是输入分布位于激活函数的非饱和区（饱和区的激活函数输出值很小，容易造成梯度弥散/爆炸）</ul><p>BN的公式： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_225611.png" alt="img" /></p><p>BN实现细节：</p><ul><li>BN默认打开<code class="language-plaintext highlighter-rouge">track_running_stats</code> ，因此每次forward时都会依据当前minibatch的统计量来更新running_mean和running_var。更新的方式通过momentum参数进行更新（默认0.1）： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_225522.png" alt="img" /></ul><p>Pytorch 0.4.1后，加入num_batches_tracked属性，目的是统计BN一共forward了多少minibatch。当momemtum被设置为None时，就由num_batches_tracked来控制历史统计量和当前minibatch的影响占比： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_225426.png" alt="img" /></p><p>BN的性能和batch size有很大的关系。batch size越大，BN的统计量也会越准。但是对于输入数据量较大的情况，单张显卡的batch size通常较小。解决该问题的一个方案是：SyncBN，即所有卡共享一个BN，得到全局的统计量。 Pytorch的SynBN实现分别在<code class="language-plaintext highlighter-rouge">torch/nn/modules/batchnorm.py</code> 和<code class="language-plaintext highlighter-rouge">torch/nn/modules/_functions.py</code> 。其中，前者主要负责检查输入的合法性，以及根据momentum等设置进行传参，调用后者。而后者则负责计算单卡统计量以及进程间通信。 具体实现中，单卡上的BN会计算单卡对应输入的均值，方差，然后做Normalize；SyncBN则会得到全局的统计量，其实现的过程为：</p><ul><li>单卡分别按照各自输入进行sum/square sum<li>进行所有卡的synchronize，得到global mean &amp; std进行同步<li>单卡按照global mean &amp; std进行各自的normalize <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_222441.png" alt="img" /></ul><p>backward，计算<code class="language-plaintext highlighter-rouge">weight</code> ，<code class="language-plaintext highlighter-rouge">bias</code> 的梯度以及<code class="language-plaintext highlighter-rouge">dy</code> ， <code class="language-plaintext highlighter-rouge">dy/du</code> 计算x的梯度，基本步骤</p><ul><li>all reduce计算梯度之和<li>根据总的size，对梯度计算平均<li>backward进行梯度计算</ul><p><a id="org3d3b613"></a></p><h1 id="torchutilsdata-解析数据处理全流程">torch.utils.data: 解析数据处理全流程</h1><p>torch的data模块核心是迭代器。其关键组件包括：</p><ul><li>Dataset：负责对raw data source进行封装，通常包含两种：Map-style datasets和iterable datasets两种，前者通过<code class="language-plaintext highlighter-rouge">__getitem__</code> 实现数据获取，后者通过<code class="language-plaintext highlighter-rouge">__iter__</code> 实现数据获取。Map-style datasets的应用较多，iterable datasets适用的情况是：随机读取的代价很大甚至不可能，且batch size取决于获取的数据。此外还包括：ConcatDatasets/ChainDataset/SubSet/TensorDataset等其他Dataset<li>Sampler：负责提供一种遍历数据集所有元素索引的方式。可支持用于自定义或者PyTorch提供。<li>DataLoader：PyTorch数据加载的核心，复杂加载数据，同时支持Map-style和Iterable-style Dataset，支持单进程/多进程，还可以设置loading order，batch size，pin memory等参数。<li>三者关系：Dataloader负责总的调度，命令Sampler定义遍历索引的方式，然后通过索引去Dataset中提取元素。</ul><p>几个核心点：</p><ul><li>批处理（默认True）：通过collate_fn实现，主要是将输入样本整理为一个batch，一般做下面三件事情：<ul><li>添加新的批次维度<li>将Numpy数组和Python数值转化为PyTorch张量<li>保留原始数据结构。</ul><li>多进程（multi-process）：<ul><li>单进程：num_workers为0，可以显示更多可读的错误跟踪，对于调试很有用。<li>多进程：num_workers控制，dataset，collate_fn，worker_init_fn都会传到每个worker中，每个worker都会用独立的进程。对于map-style数据，shuffle会在主线程完成，然后将用Sampler产生的index传到每个worker中。对于iterable-style数据，因为每个worker都有相同的data复制样本，并在每个进程中进行不同操作，所以需要通过get_worker_info()来进行辅助处理防止每个进程输出的数据重复。另外，由于多进程使用CUDA和共享CUDA张量的时候可能发生问题，建议采用pin_memory=True，以使数据能够快速传输到支持CUDA的GPU，不建议在使用多线程的情况下返回CUDA的tensor。</ul><li>Memory Pinning:<ul><li>锁页内存：锁页内存存放的内容在任何情况都不会和虚拟内存进行交换（虚拟内存指硬盘）。非锁页内存则会在内存不足的时候，将数据存放在虚拟内存中。<code class="language-plaintext highlighter-rouge">pin_memory=True</code> 时，内存中的Tensor转义到GPU的显存就会更快些。</ul><li>Prefetch：<ul><li>Dataloader通过指定<code class="language-plaintext highlighter-rouge">prefetch_factor</code> (默认为2)来进行数据的预取。其适用于多进程加载。</ul></ul><p><a id="org0922dff"></a></p><h1 id="nnmodule-核心网络模块接口">nn.Module: 核心网络模块接口</h1><p>前置介绍Pytorch中的几个关键名词概念：parameter/buffer，引用自：<a href="https://zhuanlan.zhihu.com/p/89442276">知乎Pytorch解读</a>。parameter指反向传播需要被optimizer更新的；buffer指反向传播不需要被optimizer更新。 torch的各个组件关系概览： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_222528.png" alt="img" /> torch模块的组织和实现的几个常见特点：</p><ul><li>基类定义接口，通过继承来处理不同维度的输入<li>每个类都有对应的<code class="language-plaintext highlighter-rouge">nn.functional</code> 函数，forward函数中将arguments/parameters传给<code class="language-plaintext highlighter-rouge">nn.functional</code> 的对应函数来实现forward功能。<li>继承<code class="language-plaintext highlighter-rouge">nn.Module</code> 的模块主要重载<code class="language-plaintext highlighter-rouge">init</code> / <code class="language-plaintext highlighter-rouge">forward</code> / <code class="language-plaintext highlighter-rouge">extra_repr</code> 函数，含有parameters的模块还会实现<code class="language-plaintext highlighter-rouge">reset_parameters</code> 函数来初始化参数。</ul><p><code class="language-plaintext highlighter-rouge">nn.Module</code> 实现：</p><ul><li>常用接口：<ul><li><code class="language-plaintext highlighter-rouge">__init__</code> ：初始化一系列重要的成员变量<li>状态转化：通过<code class="language-plaintext highlighter-rouge">self.training</code> 来区分训练/测试两种状态<li>参数转化/转移：包括CPU/GPU的转移以及不同类型的转化，主要通过<code class="language-plaintext highlighter-rouge">self._apply</code> (function)来实现的。<code class="language-plaintext highlighter-rouge">self._apply</code> 包括三个步骤：<code class="language-plaintext highlighter-rouge">self.children()</code> 来实现递归调用对<code class="language-plaintext highlighter-rouge">self._parameters</code> 中的参数及其gradient通过function进行处理对<code class="language-plaintext highlighter-rouge">self._buffers</code> 中的buffer逐个通过function进行处理。</ul><li>属性的增删改查：<ul><li>属性修改包含三个函数：<code class="language-plaintext highlighter-rouge">add_module</code> (增加子神经网络模块)/<code class="language-plaintext highlighter-rouge">register_parameter</code> (增加通过BP更新的parameter)/<code class="language-plaintext highlighter-rouge">register_buffer</code> (增加不通过BP更新的buffer)。常用的方式为：<code class="language-plaintext highlighter-rouge">self.xxx = xxx</code> 来进行属性的增加/修改，本质上是调用<code class="language-plaintext highlighter-rouge">nn.Module</code> 重载的函数 <code class="language-plaintext highlighter-rouge">__setattr__</code> ;但是需要注意的是：<code class="language-plaintext highlighter-rouge">self.xxx = torch.Tensor()</code> 是一种不被推荐的行为，因为该行为新增的attribute不属于<code class="language-plaintext highlighter-rouge">self._parameters</code>/<code class="language-plaintext highlighter-rouge">self._buffers</code>，而是被认为是普通的attribue，在进行状态转化时，self.xxx会被遗漏而导致device或者type不一样的bug。<li>属性删除：<code class="language-plaintext highlighter-rouge">__delattr__</code> 实现，会挨个检查<code class="language-plaintext highlighter-rouge">self._parameters</code> ，<code class="language-plaintext highlighter-rouge">self._buffers</code> ，<code class="language-plaintext highlighter-rouge">self._modules</code> 和普通的attribute并将name从中删除。</ul><li>Forward &amp; Backward<ul><li>Hooks：nn.Module类实现了3个通用的hook注册函数，用于注册被应用于全局的hook。该三个函数分别注册进3个全局的OrderedDict：_<code class="language-plaintext highlighter-rouge">global_backward_hooks</code> / <code class="language-plaintext highlighter-rouge">_global_forward_pre_hooks</code> / <code class="language-plaintext highlighter-rouge">_global_forward_hooks</code> 。同时，<code class="language-plaintext highlighter-rouge">nn.Module</code> 也支持注册应用于自身的forward/backward hook：<code class="language-plaintext highlighter-rouge">self._backward_hooks</code> / <code class="language-plaintext highlighter-rouge">self._forward_pre_hooks</code> / <code class="language-plaintext highlighter-rouge">self._forward_hooks</code> 。各个hook的调用顺序如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_222715.png" alt="img" /></ul><li>模块存取：<ul><li>Hooks：<code class="language-plaintext highlighter-rouge">_register_state_dict_hook</code> ：在<code class="language-plaintext highlighter-rouge">self.state_dict()</code> 的最后对模块导出的state_dict进行修改；<code class="language-plaintext highlighter-rouge">_register_load_state_dict_pre_hook</code> ：在_<code class="language-plaintext highlighter-rouge">load_from_state_dict</code> 中最先被执行。<li>实现细节：<ul><li><code class="language-plaintext highlighter-rouge">state_dict()</code> 可用于获取模型当前的状态信息。其中，版本信息（<code class="language-plaintext highlighter-rouge">_version</code> ）存于metadata中。数据保存通过<code class="language-plaintext highlighter-rouge">_save_to_state_dict()</code> 实现，保存内容包括<code class="language-plaintext highlighter-rouge">self._parameters</code> 以及<code class="language-plaintext highlighter-rouge">self._buffers</code> 中的persistent buffer。<li><code class="language-plaintext highlighter-rouge">load_state_dict()</code> 函数用于读取checkpoint。本质上是调用<code class="language-plaintext highlighter-rouge">_load_from_state_dict</code> 函数来加载所需权重。每个子模块可以重定义自身的<code class="language-plaintext highlighter-rouge">_load_from_state_dict</code> 函数，来避免<code class="language-plaintext highlighter-rouge">BC-breaking</code> （向后兼容）等问题。</ul></ul></ul><p><a id="org69d7f8e"></a></p><h1 id="dp--ddp-模型并行和分布式训练">DP &amp; DDP: 模型并行和分布式训练</h1><ul><li>DP:<ul><li>基本使用：<code class="language-plaintext highlighter-rouge">model=nn.DataParallel</code> (model).<li>原理：DP基于单机多卡，其中device[0]负责梯度整合和传播。包含三个核心过程：各卡分别计算损失和精度；所有梯度整合到device[0]；device[0]进行参数更新，同时其他卡拉取device[0]参数进行参数更新。其中各卡计算损失和精度的过程是并行的。<li>实现：前向传播时，通过Scatter函数将数据从device[0]分配并复制到不同的卡，后用Replicate函数将模型从device[0]复制到不同的卡，然后各卡分别调用forward计算损失和梯度。反向传播的时候，通过gather函数将梯度收集到device[0]然后在device[0]更新参数。</ul><li>DDP:<ul><li>基本使用：</ul></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># 初始化后端
</span><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">'nccl'</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">ws</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s">'env://'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s">'cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># 这里通过规定device_id采用了单卡单进程
</span><span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>-   原理：DDP也是数据并行的方式，即每张卡中都有模型和输入。通过Reducer来管理梯度同步。通过构建时注册autograd hook来进行梯度同步，当一个梯度计算好后，相应的hook会通知DDP进行规约。当某一梯度值满后（对应单个bucket），Reducer会启动异步的allreduce去计算所有进程的平均值。整个过程中，DDP可以边计算边通信，提高效率。当所有的bucket的梯度都完成计算后，Reducer会等所有allreduce完成，然后将得到的梯度更新：写到param.grad。
-   实现：backend通常采用NCCL，不过只支持GPU Tensor的通信。具体组成主要包含了三个部分：
    -   constructor：负责将rank 0的state_dict()广播，保证网络的初始状态一致；初始化buckets,控制parameters在buckets的逆序排列，提高桶通信的效率；为每个parameter加上grad_accumulator以及在autograd_graph注册autograd_hook，负载在backward时进行梯度同步
    -   forward：包含了正常的forward操作以及检查unused_parameters的操作：DDP会在forward结束时对traverse autograd graph找到没用过的parameters并标记为ready：开销很大，但是在动态图中是很有必要的，因为动态图可能会发生改变。
    -   autograd_hook：挂在autograd graph中负责在backward时负责梯度同步。 ```
</pre></table></code></div></div><p><a id="org62c086e"></a></p><h1 id="torchoptim-优化算法接口">torch.optim: 优化算法接口</h1><p>基本概念：</p><ul><li>组成部分： 优化器：训练阶段模型可学习参数的更新策略。 学习率：lr<li>调用过程：<code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> 清空梯度-&gt;<code class="language-plaintext highlighter-rouge">loss.backward()</code> 反向传播-&gt;<code class="language-plaintext highlighter-rouge">optimizer.step()</code> 更新模型参数。</ul><p>简单的调用示例：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre><td class="rouge-code"><pre>    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span> <span class="c1"># ignore warnings
</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nb">pow</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>

    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'No.{: 5d}, loss: {:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># 梯度清零
</span>            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 反向传播计算梯度
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># 梯度下降法更新参数
</span>            <span class="n">No</span><span class="p">.</span>  <span class="mi">100</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">26215.714844</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">200</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">11672.815430</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">300</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">4627.826172</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">400</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">1609.388062</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">500</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">677.805115</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">600</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">473.932159</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">700</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">384.862396</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">800</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">305.365143</span>
            <span class="n">No</span><span class="p">.</span>  <span class="mi">900</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">229.774719</span>
            <span class="n">No</span><span class="p">.</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">161.483841</span>
</pre></table></code></div></div><p>实现细节：</p><ul><li>optimizer：<ul><li>支持的optimizer：SGD/ASGD/Adadelta/Adagrad/Adam/AdamW/Adamax/SparseAdam/RMSprop/Rprop/LBFGS<li>基类Optimizer：公用方法包括：add_param_group/step/zero_grad/state_dict/load_state_dict。其中step方法由子类（具体的optimizer类）实现。</ul><li>lr_scheduler：<ul><li>支持的lr_scheduler：StepLR/MultiStepLR/ExponentialLR/ReduceLROnPlateau/CyclicLR/OneCycleLR/CosineAnnealingLR/CosineAnnealingWarmRestarts/LambdaLR/MultiplicativeLR<li>基类_LRScheduler：公用方法包括：step/get_lr/get_last_lr/print_lr/state_dict/load_state_dict。其中get_lr需要子类实现，state_dict和load_state_dict可能子类会重写。step函数为核心方法，该方法的调用逻辑为：last_epoch进行自增，然后调用子类的get_lr方法获得该epoch时的学习率，并更新到optimizer的param_groups属性之中，最后记录下最后一次调整的学习率到<code class="language-plaintext highlighter-rouge">self._last_lr</code> 。</ul></ul><p><a id="orga911203"></a></p><h1 id="torchcudaamp-自动混合精度">torch.cuda.amp: 自动混合精度</h1><p>自动混合精度训练：训练FP32模型时，一部分算子操作精度为FP16，其余算子精度为FP32，而具体哪些算子用FP16还是FP32，通过amp自动安排实现。该方案的优势是：不改变模型，不降低模型训练精度的前提下，可以 <strong>缩短训练时间，降低存储要求</strong> 。因此能够支持更多的batch size/更大模型和尺寸更大的输入进行训练。</p><ol><li>混合精度训练机制<ul><li>amp机制包含两个核心功能：自动选择合适的数值精度/对于FP16的梯度数值溢出问题，提供梯度scaling操作。<li>autocast作为Python上下文管理器和装饰器来使用，用于指定脚本中某个区域，或者某些函数，按照自动混合精度来运行。其操作过程如下（训练过程中拷贝为FP16模型，FP16算子基于FP16数据进行操作，FP32算子输入输出仍为FP6，计算精度为FP32，之后基于混合精度进行反向传播得到FP16梯度，最后将FP16梯度和FP32参数进行参数更新）： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog/2021-07-04-Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/Pytorch%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB_20210704_222558.png" alt="img" /><li><p>scaling操作：FP16精度的表示范围较窄，存在问题是大量非0梯度会遇到溢出问题。解决方案为：对梯度乘2<sup>N系数</sup>，称之为scale factor，将梯度shift到FP16的表示范围。该操作通过GradScaler实现，其功能在于反向传播前给loss乘scale factor，然后在梯度更新前进行unscale操作。最终amp的训练流程可归纳为：</p><ul><li>维护一个FP32数值精度模型的副本<li>在每个iteration<ul><li>拷贝并且转换为FP16模型<li>前向传播（FP16的模型）<li>loss乘scale factor s<li>反向传播（FP16的模型参数和参数梯度）<li>参数梯度乘 1/s（unscaling）<li>利用FP16的梯度更新FP32的模型参数</ul></ul><p>对于scale factor的选取的动态策略：根据loss动态变化。GradScaler涉及的scale factor每隔N个iteration乘一个大于1的系数，再scale loss；并且每次更新前检查溢出问题（inf/nan），如果有，scale factor乘一个小于1的系数并跳过iteration的参数更新环节，如果没有，则正常更新参数。训练流程如下：</p><ul><li>维护一个FP32数值精度模型的副本<li>初始化s<li>在每个iteration<ol><li>拷贝并且转换成FP16模型<li>前向传播（FP16的模型参数）<li>loss乘scale factor s<li>反向传播（FP16的模型参数和参数梯度）<li>检查有无inf/nan的参数梯度<ul><li>有：降低s，回到步骤1<li>无：继续至6</ul><li>参数梯度乘1/s<li>利用梯度更新FP32的模型参数</ol></ul></ul><li>实现细节<ul><li>API调用，基本操作如下：<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>    <span class="c1"># amp依赖Tensor core架构，所以model参数必须是cuda tensor类型
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="p">...)</span>
    <span class="c1"># GradScaler对象用来自动梯度缩放
</span>    <span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
        <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># 在autocast enable区域运行forward
</span>            <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
                <span class="c1"># model做一个FP16的副本，forward
</span>                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="c1"># 用scaler, scale loss(FP16), backward得到scaled的梯度(FP16)
</span>            <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># scaler更新参数，会先自动unscale梯度
</span>            <span class="c1"># 如果有nan或inf，自动跳过
</span>            <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="c1"># scaler factor更新
</span>            <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
</pre></table></code></div></div></ul><li>autocast类：作为上下文管理器和装饰器来使用，为算子自动安排按照FP16/FP32进行运算。autocast算子包含三类：FP16/FP32/按照FP16-FP32更大精度操作。对于autocast的enable区域 <strong>计算</strong> 得到的FP16数值精度变量在enable区域需要显示转化为FP32。另外，autocast作为装饰器使用（通常对于data parallel模型），设计为“thread local”，所以在main thread上设autocast区域是不work的。正确的姿势是对forward进行装饰:<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>    <span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="p">...</span>
        <span class="o">@</span><span class="n">autocast</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
            <span class="p">...</span>
</pre></table></code></div></div><p>或者在forward函数中设autocast区域：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>    <span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="p">...</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
                <span class="p">...</span>
</pre></table></code></div></div><p>对于用户自定义的autograd函数，需要用<code class="language-plaintext highlighter-rouge">amp.custom_fwd</code> 装饰forward函数，<code class="language-plaintext highlighter-rouge">amp.custom_bwd</code> 装饰backward函数。</p><li>GradScaler类：几个关键方法包括：scale(output)/step/update：<ul><li><p>scale：对outputs乘scale factor，并返回，如果enable=False，就原样返回。</p><li><p>step：梯度unscale，如果之前未手动调用unscale方法的话；检查梯度溢出，如果没有nan/inf，就执行optimizer的step，如果有就跳过。</p><li><p>update：在每个iteration结束前调用，如果参数更新则跳过，否则会将scale factor乘以backoff_factor。或者到了该增长的iteration时，将scale factor乘growth_factor，或者用new_scale直接更新scale factor。简单的使用样例：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>    <span class="c1"># Gradient clipping
</span>    <span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
        <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="n">scaler</span><span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># unscale梯度，可以不影响clip的threshold
</span>            <span class="n">scaler</span><span class="p">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

            <span class="c1"># clip梯度
</span>            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="p">)</span>

            <span class="c1"># unscale_()之前已被显式调用，scaler正常执行step更新参数，有nan/inf则会跳过
</span>            <span class="n">scaler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">scaler</span><span class="p">.</span><span class="n">update</span><span class="p">()</span>
</pre></table></code></div></div></ul></ol><p><a id="orge28e334"></a></p><h1 id="cpp_extension-ccuda算子实现和调用全流程">cpp_extension: C++/CUDA算子实现和调用全流程</h1><p>Pytorch的C++/CUDA扩展能够对功能模块进行加速。下文以nms的实现来解释C++/CUDA算子的调用流程。 调用nms的样例如下：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre>    <span class="kn">from</span> <span class="nn">mmcv</span> <span class="kn">import</span> <span class="n">_ext</span> <span class="k">as</span> <span class="n">ext_module</span>
    <span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>

    <span class="k">def</span> <span class="nf">nms</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">inds</span> <span class="o">=</span> <span class="n">NMSop</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
        <span class="n">dets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">boxes</span><span class="p">[</span><span class="n">inds</span><span class="p">],</span> <span class="n">scores</span><span class="p">[</span><span class="n">inds</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dets</span><span class="p">,</span> <span class="n">inds</span>

    <span class="k">class</span> <span class="nc">NMSop</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
        <span class="o">@</span><span class="nb">staticmethod</span>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
            <span class="n">inds</span> <span class="o">=</span> <span class="n">ext_module</span><span class="p">.</span><span class="n">nms</span><span class="p">(</span>
                <span class="n">bboxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">iou_threshold</span><span class="p">),</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">inds</span>

        <span class="o">@</span><span class="nb">staticmethod</span>
        <span class="k">def</span> <span class="nf">symbolic</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">bboxes</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
            <span class="k">pass</span> <span class="c1"># onxx转换相关
</span></pre></table></code></div></div><p>NMSop中的核心部分调用了<code class="language-plaintext highlighter-rouge">mmcv._ext.nms</code> ，存在于<code class="language-plaintext highlighter-rouge">mmcv/_ext.cpython-xxx.so</code> 文件中，通过 <code class="language-plaintext highlighter-rouge">MMCV_WITH_OPS=True python setup.py build_ext --inplace</code> 编译得到。 其中，setup.py入口为 <code class="language-plaintext highlighter-rouge">setup</code> 函数，该函数的一个主要参数为 <code class="language-plaintext highlighter-rouge">ext_modules</code> ，通过 <code class="language-plaintext highlighter-rouge">get_extensions</code> 函数得到，该函数会分别基于不同环境：C++/CUDA调用不同的拓展：CppExtension/CUDAExtension。两个拓展函数会将系统目录的库/头文件加入默认的编译搜索路径中，同时补充其他编译信息，最后生成gcc/nvcc命令：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>    <span class="n">setup</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s">'mmcv'</span><span class="p">,</span>
        <span class="n">install_requires</span><span class="o">=</span><span class="n">install_requires</span><span class="p">,</span>
        <span class="c1"># 需要编译的c++/cuda扩展
</span>        <span class="n">ext_modules</span><span class="o">=</span><span class="n">get_extensions</span><span class="p">(),</span>
        <span class="c1"># cmdclass为python setup.py --build_ext命令指定行为
</span>        <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span><span class="s">'build_ext'</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">cpp_extension</span><span class="p">.</span><span class="n">BuildExtension</span><span class="p">}</span>
    <span class="p">)</span>
</pre></table></code></div></div><p>编译完成后，需要进行将C++/CUDA的二进制文件和Python进行连接，主要通过pybind11库实现，该库是用于在C++代码中创建Python的连接的库。pybind.cpp中的核心代码如下：</p><div class="language-c++ highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>    <span class="cp">#include &lt;torch/extension.h&gt;
</span>    <span class="c1">// 函数声明，具体实现在其他文件</span>
    <span class="n">Tensor</span> <span class="nf">nms</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">Tensor</span> <span class="n">scores</span><span class="p">,</span> <span class="kt">float</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="kt">int</span> <span class="n">offset</span><span class="p">);</span>

    <span class="c1">// TORCH_EXTENSION_NAME在编译命令中传入为_ext，表示了声明了名为_ext的Python module</span>
    <span class="c1">// nms定义为_ext下的子模块</span>
    <span class="n">PYBIND11_MODEL</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"nms"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">nms</span><span class="p">,</span> <span class="s">"nms (CPU/CUDA)"</span><span class="p">,</span> <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"boxes"</span><span class="p">),</span> <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"scores"</span><span class="p">),</span>
              <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"iou_threshold"</span><span class="p">),</span> <span class="n">py</span><span class="o">::</span><span class="n">arg</span><span class="p">(</span><span class="s">"offset"</span><span class="p">));</span>
    <span class="p">}</span>
</pre></table></code></div></div><p>接下来便是实现具体的C++/CUDA算子，以下示例C++实现：</p><div class="language-c++ highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre>    <span class="cp">#include &lt;torch/extension.h&gt;
</span>    <span class="k">using</span> <span class="k">namespace</span> <span class="n">at</span><span class="p">;</span> <span class="c1">// ATen（A Tensor Library），为python扩展c++负责声明和定义Tensor运算的相关逻辑库</span>
    <span class="n">Tensor</span> <span class="nf">nms_cpu</span><span class="p">(</span><span class="n">Tensor</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">Tensor</span> <span class="n">scores</span><span class="p">,</span> <span class="kt">float</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="kt">int</span> <span class="n">offset</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// 仅显示核心代码</span>
        <span class="k">for</span> <span class="err">（</span><span class="kt">int64_t</span> <span class="n">_i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">_i</span> <span class="o">&lt;</span> <span class="n">nboxes</span><span class="p">,</span> <span class="n">_i</span><span class="o">++</span><span class="err">）</span> <span class="p">{</span>
                <span class="c1">// 遍历所有检测框，称为主检测框</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">select</span><span class="p">[</span><span class="n">_i</span><span class="p">]</span> <span class="o">==</span> <span class="nb">false</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">_j</span> <span class="o">=</span> <span class="n">_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">_j</span> <span class="o">&lt;</span> <span class="n">nboxes</span><span class="p">;</span> <span class="n">_j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                    <span class="c1">// 对每个主检测框，遍历其他检测框，称为次检测框</span>
                    <span class="c1">// 这里只用遍历上三角元素即可，节省计算</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">select</span><span class="p">[</span><span class="n">_j</span><span class="p">]</span> <span class="o">==</span> <span class="nb">false</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
                    <span class="k">auto</span> <span class="n">ovr</span> <span class="o">=</span> <span class="n">inter</span> <span class="o">/</span> <span class="p">(</span><span class="n">iarea</span> <span class="o">+</span> <span class="n">area</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">inter</span><span class="p">);</span>
                    <span class="c1">// 如果次检测框和主检测框iou过大，则去除次检测框</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">ovr</span> <span class="o">&gt;=</span> <span class="n">iou_threshold</span><span class="p">)</span> <span class="n">select</span><span class="p">[</span><span class="n">_j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="k">return</span> <span class="n">order_t</span><span class="p">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">select_t</span><span class="p">);</span>
    <span class="p">}</span>
</pre></table></code></div></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AE%97%E6%B3%95%E9%83%A8%E7%BD%B2/'>算法部署</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >Pytorch</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Pytorch关键模块解读 - Johney Zheng&url=https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Pytorch关键模块解读 - Johney Zheng&u=https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Pytorch关键模块解读 - Johney Zheng&url=https://www.johneyzheng.top//posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E6%A0%88/">大模型推理技术栈</a><li><a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">模型部署技术概览</a><li><a href="/posts/FlashAttention%E7%B3%BB%E5%88%97%E4%BC%98%E5%8C%96/">FlashAttention系列优化</a><li><a href="/posts/Win10_Ubuntu_installation/">双系统安装(WIN10+Ubuntu16)</a><li><a href="/posts/nms_soft-nms/">nms and soft-nms</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>接下来阅读</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ONNX%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E4%B8%8E%E9%87%8F%E5%8C%96%E7%BB%86%E8%8A%82/"><div class="card-body"> <span class="timeago small" > Sep 21, 2021 <i class="unloaded">2021-09-21T17:18:48+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ONNX的模型优化与量化细节</h3><div class="text-muted small"><p> ONNX的模型优化与量化细节 ONNX基本介绍 什么是ONNX？ ONNX全称为 Open Neural Network Exchange，是一种与框架无关的模型表达式。ONNX的规范及代码主要由微软，亚马逊 ，Facebook 和 IBM 等公司共同开发，以开放源代码的方式托管在Github上。目前官方支持加载ONNX模型并进行推理的深度学习框架有： Caff...</p></div></div></a></div><div class="card"> <a href="/posts/Transformer%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-GPU%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/"><div class="card-body"> <span class="timeago small" > May 14, 2022 <i class="unloaded">2022-05-14T16:27:50+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Transformer离线部署-GPU优化策略</h3><div class="text-muted small"><p> 前言 模型结构分析 具体优化措施 参考资料 前言 本文主要介绍Transformer类网络在GPU设备上部署上的优化要点。 主要围绕Nvidia开源的FasterTransformer展开。 模型结构分析 标准的Transformer结构主要包括 Encoder 和 Decoder 两部分结构，具体结构分析可参考Transformer在CV领域的应用与部署： ...</p></div></div></a></div><div class="card"> <a href="/posts/Goolge-TPU%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"><div class="card-body"> <span class="timeago small" > Jul 17, 2022 <i class="unloaded">2022-07-17T16:30:50+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Goolge-TPU论文解读</h3><div class="text-muted small"><p> 文章背景 摘要 设计方案 性能结果 Discussion &amp;&amp; Conclusion 个人观点 参考 文章背景 In-Datacenter Performance Analysis of a Tensor Processing Unit 发表于ISCA2017，主要介绍Google TPU的架构。 摘要 Google从2013年发现，大...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%E5%9F%BA%E4%BA%8EJekyll%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" class="btn btn-outline-primary" prompt="较早文章"><p>基于Jekyll搭建博客</p></a> <a href="/posts/Python-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" class="btn btn-outline-primary" prompt="较新文章"><p>Python-基本数据结构</p></a></div><div id="comments"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ el: '#comments', app_id: 'IJm2s0GdkzhEOLwVfClrHeWs-gzGzoHsz', app_key: 'Y281bajarkkIGs8p4WmrTkNi', placeholder: '请在下面评论：', visitor: true }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">Johney Zheng</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，否则本网站上的博客文章均由作者根据知识共享许可协议 - 署名标示 4.0（CC BY 4.0）进行授权许可。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.johneyzheng.top/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
