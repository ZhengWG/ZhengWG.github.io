<!DOCTYPE html><html lang="zh-Hans" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="AI模型部署硬件综述" /><meta property="og:locale" content="zh_Hans" /><meta name="description" content="硬件篇 GPU NVIDIA 壁仞科技 XPU Google-TPU 晟腾 昆仑 寒武纪 软件篇 核心内容 参考" /><meta property="og:description" content="硬件篇 GPU NVIDIA 壁仞科技 XPU Google-TPU 晟腾 昆仑 寒武纪 软件篇 核心内容 参考" /><link rel="canonical" href="https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/" /><meta property="og:url" content="https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/" /><meta property="og:site_name" content="Johney Zheng" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-11-13T21:30:50+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="AI模型部署硬件综述" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"硬件篇 GPU NVIDIA 壁仞科技 XPU Google-TPU 晟腾 昆仑 寒武纪 软件篇 核心内容 参考","url":"https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/","headline":"AI模型部署硬件综述","dateModified":"2022-11-13T21:30:50+08:00","datePublished":"2022-11-13T21:30:50+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/"},"@context":"https://schema.org"}</script><title>AI模型部署硬件综述 | Johney Zheng</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Johney Zheng"><meta name="application-name" content="Johney Zheng"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-364499169"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-364499169'); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Johney Zheng</a></div><div class="site-subtitle font-italic">Johney Zheng的小站</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/agents/" class="nav-link"> <i class="fa-fw fas fa-robot ml-xl-3 mr-xl-3 unloaded"></i> <span>AGENTS</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/ZhengWG" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['21625111','zju.edu.cn'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>AI模型部署硬件综述</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>AI模型部署硬件综述</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Johney Zheng </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Nov 13, 2022, 9:30 PM +0800" prep="on" > Nov 13, 2022 <i class="unloaded">2022-11-13T21:30:50+08:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2784 words">15 min</span></div></div><div class="post-content"><ul><li><a href="#sec-1">硬件篇</a><ul><li><a href="#sec-1-1">GPU</a><ul><li><a href="#sec-1-1-1">NVIDIA</a><li><a href="#sec-1-1-2">壁仞科技</a></ul><li><a href="#sec-1-2">XPU</a><ul><li><a href="#sec-1-2-1">Google-TPU</a><li><a href="#sec-1-2-2">晟腾</a><li><a href="#sec-1-2-3">昆仑</a><li><a href="#sec-1-2-4">寒武纪</a></ul></ul><li><a href="#sec-2">软件篇</a><ul><li><a href="#sec-2-1">核心内容</a></ul><li><a href="#sec-3">参考</a></ul><h1 id="硬件篇">硬件篇<a id="sec-1"></a></h1><h2 id="gpu">GPU<a id="sec-1-1"></a></h2><p>GPU（Graphics Processing Unit），全称为图形处理器，是一种在个人电脑、工作站和一些移动设备上做图像和图形相关运算的微处理器。最早由NVIDIA公司在1999年8月发表GeForce 256芯片时提出的概念。早先，GPU的功能更多地应用在图像计算（尤其是三维绘图运算），如硬件坐标转换、立体环境材质贴图等。GPU根据接入方式可以划分为独立GPU和集成GPU。独立GPU一般封装在独立的显卡电路板上，拥有独立显存；而集成GPU通常会和CPU共用资源，如系统内存。根据应用端区别，GPU可以分为PC/服务器/移动端GPU。</p><p>当前传统GPU市场（PC/服务器侧）基本被国外厂商垄断，前三厂商Nvidia/AMD/Intel的营收收入基本占据了整个GPU行业收入，其中INtel的营业收入主要来源于集成显卡。移动端GPU，如手机/平板GPU，头部厂商主要为ARM/高通/苹果。就行业而言，GPU在过去的20多年来的主要需求来源于视频加速、2D/3D游戏；但是随着GPU在并行处理和通用计算上的优势，在人工智能、边缘计算领域等逐渐迎来爆发。就NVIDIA而言，2021年营收收入为167亿美元，其中游戏收入占比47%，数据中心收入占比40%，专业视觉/自动驾驶业务占比9%。对于计算产业而言，衡量GPU的维度主要是通用性、易用性和高性能：即硬件框架需要足够灵活；开发门槛低，易于上手；芯片基本性能和性价比高。 国产GPU发展相对滞后，最早的GPU芯片JM5400最早由景嘉微于2014年开发。其他较为知名的厂商包括：壁仞科技、芯原股份、登临科技、海思等。</p><h3 id="nvidia">NVIDIA<a id="sec-1-1-1"></a></h3><p>对于大型数据中心、人工智能、超算领域，高端GPU逐渐成为算力刚需。NVIDIA在高端GPU的市场份额占比超过90%。截止当前，NVIDIA已经推出了Volta、Ampere、Hopper等用于高性能计算和AI训练架构，对应的GPU包括V100、A100、H100等，面向向量的双精度浮点运算能力从7.8TFLOPS到30TFLOPS。下文重点介绍下NVIDIA最新的Hopper架构。NVIDIA近10年的GPU微架构演变为：Tesla-&gt;Fermi-&gt;Kepler-&gt;Maxwell-&gt;Pascal-&gt;Volta-&gt;Turing-&gt;Ampere-&gt;Hopper。</p><p>介绍Hopper架构之前，先简单介绍下NVIDIA GPU的基本单元SM(streaming-multiprocessor)，其基本结构如下（不同架构下的SM单元组成存在部分区别）：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221005_164310.png" alt="img" /></p><p>SM单元被认为是GPU的心脏，几个核心单元包括：</p><ul><li>CUDA Core: Fermi架构前也被称之为SP(scalar processor core)，为通用计算单元，能够通过单指令实现“乘加运算”。根据支持的数据类型，分为INT32、FP32、FP64 Cuda Core单元。<li>Tensor Core: Volta架构后引入，主要用于矩阵运算（MMA），支持FP16/FP32混合精度<li>LD/ST： 全称Load/Store Unit，访存单元<li>SFU：全称Special function unit，特殊数学计算单元，如sin/cos<li>Warp Scheduler: warp为NVIDIA线程调度基本单位，每个SM都有wrap scheduler用于wrap调度<li>Dispatch Unit: 负责将wrap scheduler指令送往core（cuda core/tensor core/LD/ST/SFU）执行<li>L0 Instruction Cache: SM单元内置缓存<li>Register File: 寄存器资源，被切分到每个SM单元</ul><p>SM单元外是张量内存加速器（TMA，Tensor Memory Accelerator）/L1缓存/Shared Memory。其中TMA可以在全局内存和共享内存之间高效地传输大块数据、异步复制数据等功能。</p><p>NVIDIA Hopper架构在<a href="https://www.nvidia.cn/lp/data-center/resources/download-hopper-arch-whitepaper/">NVIDIA架构白皮书</a>上有更为详细的说明，这里做重点内容的梳理。</p><ol><li>4nm工艺，H100而言，SXM4外形尺寸下功耗为700W，内存带宽3TB/s，相对A100，H100时钟速度增加30%，SM数量增加1.2倍<li>SM单元关键特性：每个SM单被组织为4个象限，每个象限包含16个INT32单元，支持混合精度的INT32/INT8/INT4处理；32个FP32单元，支持FP16/FP32混合精度；16个FP64单元；384个32位寄存器；8个LD/ST单元；4个SFU单元；1个Tensor Core<li><p>Tensor Core关键特性：采用了第四代Tensor Core架构，运算吞吐量提升一倍。Tensor Core支持FP8/FP16/BF16/TF32/FP64和INT8 MMA数据类型。其中，FP8 Tensor Core为新增数据类型，能够支持多个累加器数据类型和输出类型（参考下图），同时可通过Transformer引擎进行FP8/FP16精度选择，保持精度的同时提升吞吐量（~2倍）。</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221006_231604.png" alt="img" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221006_231829.png" alt="img" /></p><li>shared memory/L1 Cache：H100 L1 Cache为256KB/SM，A100为192KB/SM，共享内存最大到228KB<li>综合性能对比：结合1中的时钟频率和SM数量，3中的Tensor Core算力提升和transformer引擎，H100相对A100的算力提升约6倍。</ol><h3 id="壁仞科技">壁仞科技<a id="sec-1-1-2"></a></h3><p>国产GPU相对公开的资料较少，以<a href="https://www.birentech.com/BR100.html">壁仞科技</a>为例，当前BR100系列包含BR100、BR104两款芯片。BR100核心参数如下：</p><ul><li>2048 TOPS INT8/1024 TOPS BF16/512 TOPS TF32+/256 TOPS FP32<li>PCIe 5.0接口 128GB/s，BLink（对标NvLink）<li>300+MB缓存，2.3TB/s外部I/O带宽，64路编码，512路解码，2.5D CoWoS封装 7nm工艺 550W功耗</ul><p>芯片架构整体框架图如下，主要包含几个部分：计算单元、2Dmesh片上网、HBM2e存储系统、媒体引擎、PCIe 5.0接口、BLink互联接口：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221008_225402.png" alt="img" /></p><p>芯片中的核心计算单元SPC，类比GPU的SM单元，每个SPC包含16个EU（Execution Unit），每个EU单元含有16个通用计算核（V-core）,和一个Tensor core，指令集层面采用的是SIMT，同时采用类NVIDIA的C-Warp协作并发，因此兼容CUDA。采用分布式shared L2cache，通过片上网连接</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221008_230952.png" alt="img" /></p><h2 id="xpu">XPU<a id="sec-1-2"></a></h2><h3 id="google-tpu">Google-TPU<a id="sec-1-2-1"></a></h3><p>Google从2016年起，第一次提出了第一代TPU（Tensor Processing Unit），其定位为：AI accelerator application-specific intergrated circuit(ASIC)。截止2022年，TPU已经发布到TPUv4： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_163930.png" alt="img" /></p><p>Google TPU最早发布于2017年，详细介绍可参照<a href="https://johneyzheng.top/posts/Goolge-TPU%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">Google-TPU论文解读</a>。其结构设计如下：主要计算单元为矩阵乘法单元Matrix Multiply，输入包含两个部分：FIFO（权重矩阵）/UB（输入数据），其输出为累加器单元（Accumulators）。同时还有专门的非线性单元/Normalize/Pool单元。TPU架构的设计哲学： <code class="language-plaintext highlighter-rouge">The philosophy of the TPU microarchitecture is to keep the matrix unit busy.</code></p><p><a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPU v4</a>的设计框架如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_220729.png" alt="img" /></p><p>每个 v4 TPU 芯片包含两个 TensorCore。每个 TensorCore 都有四个 MXU、一个矢量单位和一个标量单位。</p><h3 id="晟腾">晟腾<a id="sec-1-2-2"></a></h3><p><a href="https://www.hiascend.com/zh/">晟腾</a>AI加速卡当前主要有Atlas 300I Duo/Atlas 300I Pro/ALtas 300V Pro三类卡。以300I Duo卡为例：</p><ul><li>7nm工艺，280 TOPS INT8，140 TFLOPS FP16，LPDDR4x 48GB，总带宽408GB/s，支持ECC，编解码能力 <strong>很强</strong> ，PCIe Gen4.0，兼容3.0/2.0/1.0</ul><p>架构层面采用的是自研的达芬奇架构，核心点在于计算单元设计（Cube/Vector/Scale Unit）和多级缓存设计（UB/L1/L2），和TPU设计思路”不谋而合：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_221926.png" alt="img" /></p><h3 id="昆仑">昆仑<a id="sec-1-2-3"></a></h3><p><a href="https://www.kunlunxin.com.cn">昆仑</a>芯片当前XPU-K/XPU-R两个架构（材料可参考：<a href="https://ieeexplore.ieee.org/document/9220641/metrics#metrics">Baidu Kunlun An AI processor for diversified workloads</a>）：</p><ul><li>第一代架构：XPU-K K200为例：14nm工艺，256 TOPS@INT8算力 64TFLOPS@FP16 PCIe 4.0x8 HBM高速显存，512GB/s内存带宽<li><p>第二代架构：XPU-R R200为例：7nm工艺，256 TOPS@INT8算力 128TFLOPS@FP16 GDDR6高性能显存 PCIe 4.0x16 <a href="https://www.kunlunxin.com.cn/122.html">XPU-R架构</a>如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221015_175351.png" alt="img" /></p></ul><h3 id="寒武纪">寒武纪<a id="sec-1-2-4"></a></h3><p><a href="https://www.cambricon.com/">寒武纪</a>芯片截止当前（2022/11），官网当前加速卡共有三代产品：思元1xx/思元2xx/思元3xx，最新的思元370芯片相关参数信息如下：</p><ul><li>思元370：7nm工艺，256 TOPS@INT8算力，MLUarch3框架，支持LPDDR5，MUL-Link互联</ul><p>对于MLUarch3架构，资料较少，其架构演变过程如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_153514.png" alt="img" /></p><p>芯粒技术（Chiplets）是涉及芯片设计制造的一种技术：将功能拆分为小块，拥有可重复使用的IP区块，可以进行模块化组合；能够增加其良品率，降低生产成本。该技术被部分厂家认为是延续摩尔定律的关键。当前很多芯片厂家都提出了各自的芯粒技术，如台积电的无凸起的系统整合单晶片（System on Integrated Chip），AMD的Zen2架构，Intel的Foveros的3D堆叠技术的异构系统集成方案。</p><p>设计架构层面，寒武纪自2014年发布了多个架构，分别为：</p><ul><li><p>DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning 首次提出了NFU（Neural Function Units）结构，包括三个部分：NFU-1，全乘法单元；NFU-2，加法树；NFU-3，激活单元。另外还有三个Buffer，分别存储输入数据、权重、计算结果。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_160240.png" alt="img" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-11-13-AI%25E6%25A8%25A1%25E5%259E%258B%25E9%2583%25A8%25E7%25BD%25B2%25E7%25A1%25AC%25E4%25BB%25B6%25E7%25BB%25BC%25E8%25BF%25B0/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0_20221113_160259.png" alt="img" /></p><li>DaDianNao: A Machine-Learning Supercomputer 为DianNao的多核版本，通过多片设计，将大模型放在芯片内存上运行。<li>ShiDianNao: Shifting Vision Processing Closer to the Sensor 集成了视频的编解码等操作<li>PuDianNao: A Polyvalent Machine Learning Accelerator DianNao收官之作，支持了7种机器学习算法：神经网络、线性模型、支持向量机、决策树、朴素贝叶斯、K临近和K类聚。<li>Cambricon-X: An Accelerator for Sparse Neural 添加了对稀疏网络的支持</ul><h1 id="软件篇">软件篇<a id="sec-2"></a></h1><p>NPU的schedule和传统CPU的schedule策略存在很大的区别，而实现的NPU高性能计算的目标便是：基于NPU策略来得到最大程度的加速。其中涉及的内容包括：内核驱动、内存管理、系统调用、上下文切换等操作系统相关的知识。很多遇到的问题也是类似的：cache利用率、DDR latency等。</p><h2 id="todo-核心内容">TODO 核心内容<a id="sec-2-1"></a></h2><h1 id="参考">参考<a id="sec-3"></a></h1><p><a href="https://zhuanlan.zhihu.com/p/491295891">Hopper架构-Transformer介绍</a></p><p><a href="https://zhuanlan.zhihu.com/p/380317994">主流手机NPU软件栈调研(2021 Q2)</a></p><p><a href="https://zhuanlan.zhihu.com/p/61735898">知乎-2018 AI云端芯片一览</a></p><p><a href="https://cloud.tencent.com/developer/article/1005802">深度学习的异构硬件加速：TPU特性与数据中心的ASIC应用</a></p><p><a href="https://cloud.tencent.com/developer/article/1006018">深度学习的异构加速技术（一）效率因通用而怠，构架为AI而生</a></p><p><a href="https://www.ancii.com/aqfl4pud/#:~:text=%E5%9C%A8%E2%80%9C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82,%E6%98%AF%E8%A7%A3%E5%86%B3%E5%B8%A6%E5%AE%BD%E9%97%AE%E9%A2%98%E3%80%82">深度学习的异构加速技术（二）：螺狮壳里做道场</a></p><p><a href="https://cloud.tencent.com/developer/article/1006375">深度学习的异构加速技术（三）：互联网巨头们“心水”这些 AI 计算平台</a></p><p><a href="https://github.com/basicmi/AI-Chip">Github-AI Chip List</a></p><p><a href="https://xueqiu.com/6027869421/231306251">英伟达NVIDIA为何可以在高性能计算GPU中处于不败地位？</a></p><p><a href="https://www.thepaper.cn/newsDetail_forward_11705617">GPU深度报告，三大巨头，十四个国内玩家一文看懂</a></p><p><a href="https://icspec.com/news/article-details/1976568">深入解读英伟达”HOPPER”GPU架构</a></p><p><a href="https://zhuanlan.zhihu.com/p/33518322">GPU基础知识</a></p><p><a href="https://zhuanlan.zhihu.com/p/423550197">英伟达GPU架构演进近10年，从费米到安培</a></p><p><a href="http://science.china.com.cn/2022-08/15/content_42070515.htm">详解壁仞刚刚发布的GPU：单芯片PFLOPS算力是怎样炼成的？</a></p><p><a href="https://picture.iczhiku.com/weixin/message1589764286036.html">从三大半导体公司（芯粒）方案看其神奇之处</a></p><p><a href="https://www.jianshu.com/p/01e2d91f260f">DianNao系列加速器总结–架构与运算单元</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AE%97%E6%B3%95%E9%83%A8%E7%BD%B2/'>算法部署</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E8%8A%AF%E7%89%87-xpu-gpu/" class="post-tag no-text-decoration" >芯片，XPU，GPU</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=AI模型部署硬件综述 - Johney Zheng&url=https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=AI模型部署硬件综述 - Johney Zheng&u=https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=AI模型部署硬件综述 - Johney Zheng&url=https://www.johneyzheng.top//posts/AI%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E7%A1%AC%E4%BB%B6%E7%BB%BC%E8%BF%B0/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Blog-Agents%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D/">Blog Agents功能介绍</a><li><a href="/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E6%A0%88/">大模型推理技术栈</a><li><a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">模型部署技术概览</a><li><a href="/posts/FlashAttention%E7%B3%BB%E5%88%97%E4%BC%98%E5%8C%96/">FlashAttention系列优化</a><li><a href="/posts/Win10_Ubuntu_installation/">双系统安装(WIN10+Ubuntu16)</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">文章目录</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>接下来阅读</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Pytorch%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97%E8%A7%A3%E8%AF%BB/"><div class="card-body"> <span class="timeago small" > Jul 4, 2021 <i class="unloaded">2021-07-04T21:42:06+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pytorch关键模块解读</h3><div class="text-muted small"><p> 目录 前言 torch.autograd: 梯度计算 BN &amp; SyncBN: BN与多卡同步BN torch.utils.data: 解析数据处理全流程 nn.Module: 核心网络模块接口 DP &amp; DDP: 模型并行和分布式训练 torch.optim: 优化算法接口 torch.cuda.amp: 自动混合精度 cpp_ext...</p></div></div></a></div><div class="card"> <a href="/posts/ONNX%E7%9A%84%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E4%B8%8E%E9%87%8F%E5%8C%96%E7%BB%86%E8%8A%82/"><div class="card-body"> <span class="timeago small" > Sep 21, 2021 <i class="unloaded">2021-09-21T17:18:48+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ONNX的模型优化与量化细节</h3><div class="text-muted small"><p> ONNX的模型优化与量化细节 ONNX基本介绍 什么是ONNX？ ONNX全称为 Open Neural Network Exchange，是一种与框架无关的模型表达式。ONNX的规范及代码主要由微软，亚马逊 ，Facebook 和 IBM 等公司共同开发，以开放源代码的方式托管在Github上。目前官方支持加载ONNX模型并进行推理的深度学习框架有： Caff...</p></div></div></a></div><div class="card"> <a href="/posts/Transformer%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-GPU%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/"><div class="card-body"> <span class="timeago small" > May 14, 2022 <i class="unloaded">2022-05-14T16:27:50+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Transformer离线部署-GPU优化策略</h3><div class="text-muted small"><p> 前言 模型结构分析 具体优化措施 参考资料 前言 本文主要介绍Transformer类网络在GPU设备上部署上的优化要点。 主要围绕Nvidia开源的FasterTransformer展开。 模型结构分析 标准的Transformer结构主要包括 Encoder 和 Decoder 两部分结构，具体结构分析可参考Transformer在CV领域的应用与部署： ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Goolge-TPU%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" class="btn btn-outline-primary" prompt="较早文章"><p>Goolge-TPU论文解读</p></a> <a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/" class="btn btn-outline-primary" prompt="较新文章"><p>模型部署技术概览</p></a></div><div id="comments"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ el: '#comments', app_id: 'IJm2s0GdkzhEOLwVfClrHeWs-gzGzoHsz', app_key: 'Y281bajarkkIGs8p4WmrTkNi', placeholder: '请在下面评论：', visitor: true }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/username">Johney Zheng</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，否则本网站上的博客文章均由作者根据知识共享许可协议 - 署名标示 4.0（CC BY 4.0）进行授权许可。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.johneyzheng.top/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
