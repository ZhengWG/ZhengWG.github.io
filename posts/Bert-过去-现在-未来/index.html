<!DOCTYPE html><html lang="zh-Hans" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Bert:过去-现在-未来" /><meta property="og:locale" content="zh_Hans" /><meta name="description" content="前言 BERT发展历程 BERT算法细节 BERT后期发展 参考资料" /><meta property="og:description" content="前言 BERT发展历程 BERT算法细节 BERT后期发展 参考资料" /><link rel="canonical" href="https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/" /><meta property="og:url" content="https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/" /><meta property="og:site_name" content="Johney Zheng" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-03T15:57:51+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bert:过去-现在-未来" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"前言 BERT发展历程 BERT算法细节 BERT后期发展 参考资料","url":"https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/","headline":"Bert:过去-现在-未来","dateModified":"2022-05-03T15:57:51+08:00","datePublished":"2022-05-03T15:57:51+08:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/"},"@context":"https://schema.org"}</script><title>Bert:过去-现在-未来 | Johney Zheng</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Johney Zheng"><meta name="application-name" content="Johney Zheng"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-364499169"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-364499169'); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Johney Zheng</a></div><div class="site-subtitle font-italic">Johney Zheng的小站</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/agents/" class="nav-link"> <i class="fa-fw fas fa-robot ml-xl-3 mr-xl-3 unloaded"></i> <span>AGENTS</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/ZhengWG" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['21625111','zju.edu.cn'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Bert:过去-现在-未来</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Bert:过去-现在-未来</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Johney Zheng </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, May 3, 2022, 3:57 PM +0800" prep="on" > May 3, 2022 <i class="unloaded">2022-05-03T15:57:51+08:00</i> </span></div><div> <span><div id="mathjax"></div><script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ["$","$"]], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'], processEscapes: true } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(); for (var i = 0; i < all.length; ++i) all[i].SourceElement().parentNode.className += ' has-jax'; }); </script> <script src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2142 words">11 min</span></div></div><div class="post-content"><ul><li><a href="#sec-1">前言</a><li><a href="#sec-2">BERT发展历程</a><li><a href="#sec-3">BERT算法细节</a><li><a href="#sec-4">BERT后期发展</a><li><a href="#sec-5">参考资料</a></ul><h1 id="前言">前言<a id="sec-1"></a></h1><p>浅谈 <code class="language-plaintext highlighter-rouge">BERT</code> 系列网络，主要从三个维度介绍：发展历程、算法细节、后期发展。 背景知识： 自然语言处理（NLP）：其最终目的是理解复杂的语言/文字，主要任务包括：</p><ul><li>序列标注：如中文分词、词性标注、命名实体识别、语义角色标注等<li>分类任务：文本分类、情感分析等<li>句子关系判断：如QA，自然语言推理等<li>生成式任务：机器翻译、文本摘要、写诗造句等。</ul><h1 id="bert发展历程">BERT发展历程<a id="sec-2"></a></h1><p>NLP本质是对语言/文字进行合理的数学表达，方便机器理解，其核心问题是怎么讲将语言文字转化为合理的“数学空间表示”。从最简单的one-hot编码到如今的BERT，都是围绕这一问题展开的：即如何学习现有的语料库（可能大部分是无标注数据），进行具体NLP任务中语言/文字的数学转换或者特征提取，而具体的任务可以认为是基于其特征的下游任务。NLP中的特征提取，类比CV场景，就是“预训练过程”，即基于 <code class="language-plaintext highlighter-rouge">ImageNet</code> 预训练的基础网络可以继续 <code class="language-plaintext highlighter-rouge">finetune</code> 供其他下游任务使用。NLP领域的预训练过程，通常采用语言模型技术（Language Model，核心是通过分析上下文来进行单词/句子的量化估计），该技术经历了几个技术发展阶段：</p><ol><li>NNLM（神经网络语言模型）：最原始的应用神经网络解决NLP问题的模型，其算法思想是：对输入句子的单词进行one-hot编码，然后乘以矩阵Q转换为向量C，将拼接的句子向量接一层 <code class="language-plaintext highlighter-rouge">hiddedn layer</code> 最后接 <code class="language-plaintext highlighter-rouge">softmax</code> ，预测下一个单词出现的概率。这里的向量C其实便是 <code class="language-plaintext highlighter-rouge">word embedding</code> 的结果。该结果相较于one-hot编码的优势是维度更低，且转换后的向量包含语义信息，比如语义更近的词，通常其距离会更小。<li>Word2Vec：Word2Vec在NNLM上更进一步，NNLM其实本身是属于单词预测的语言任务模型，不是专门用于 <code class="language-plaintext highlighter-rouge">word embedding</code> 。Word2Vec采用了和NNLM类似的网络结构，但是采用了不同的训练方法： <code class="language-plaintext highlighter-rouge">CBOW(Continuous Bag-of-Words Model)</code> 和 <code class="language-plaintext highlighter-rouge">Skip-gram(Continuous Skip-gram Model)</code> ，其中，CBOW是从句子扣掉一个词，然后根据上下文（context）预测这个词，而Skip-gram则是从单个单词去预测上下文。<li><p>ELMO（Embedding from Language Models）：Word2Vec/Glove这类语言模型的局限在于无法解决单词多义的问题，即单个单词转换的到的向量值都是恒定的，而不能根据上下文体现多义性，如 <code class="language-plaintext highlighter-rouge">play music</code> 和 <code class="language-plaintext highlighter-rouge">play football</code> ， <code class="language-plaintext highlighter-rouge">play</code> 的含义是不一致的。ELMO采用的思路是添加上下文单词语义的embedding特征来对单一的word emdding特征进行补充，其基本结构如下：</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_143946.png" alt="img" /></p><p>ELMO输入为对应单词的word emdding，训练网络包含左右两部分“双层LSTM结构”。左边结构为从前往后的正向句子输入，右边为从后往前的逆向句子输入。其中两层LSTM结构对应的是句子的句法特征和语义特征。实际使用过程中，先基于ELMO得到句子的三类embedding特征，然后可以学习三类特征的各自权重，得到加权的最终特征后可以供后续下游任务使用。 ELMO的优点非常明显，引入LSTM/RNN并且通过双向输入补充前后文信息来解决多义性问题；其缺点在于LSTM/RNN结构无法并行，训练/推理效率低，且采用多个emdding特征组合这种非 <code class="language-plaintext highlighter-rouge">end-to-end</code> 的方式，效果不如后续一体化融合的方案好。</p><li><p>GPT（Generative Pre-Training）：GPT的结构最早将 <code class="language-plaintext highlighter-rouge">Transformer</code> 结构应用于语言模型中，关于 <code class="language-plaintext highlighter-rouge">Transformer</code> 的介绍可以参考 <a href="https://johneyzheng.top//posts/Transformer%E5%9C%A8CV%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E9%83%A8%E7%BD%B2/#sec-2">Transformer在CV领域的应用</a>，其优势是提取长依赖特征更好，且并行效率更高。GPT的结构如下： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_150153.png" alt="img" /></p><p>从结构上看，GPT与ELMO的区别是：采用Transformer替代RNN；采用的是单向的结构（即只利用了上文信息）；抛弃了ELMO的多个embedding特征的区分，GPT采用的是端到端的推理方式。关于最后一点，指的是GPT的下游任务需要基于GPT的原始结构进行设计，并采用GPT预训练得到的权重重新”finetune”，这点和CV场景很类似。GPT的几种下游任务网络推荐： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_150711.png" alt="img" /></p><li><p>BERT：BERT可以认为是GPT的升级版，从结构上可以看出： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_150838.png" alt="img" /></p><p>其区别在于BERT采用了双向语言模型结构（通过Mask的方式，设计思路上类似CBOW），添加了 <code class="language-plaintext highlighter-rouge">Next Sentence Prediction</code> 多任务，且其训练规模更大。</p></ol><h1 id="bert算法细节">BERT算法细节<a id="sec-3"></a></h1><p>BERT网络结构的基本组成单元为 <code class="language-plaintext highlighter-rouge">Transformer</code> 结构，<a href="https://johneyzheng.top//posts/Transformer%E5%9C%A8CV%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8%E4%B8%8E%E9%83%A8%E7%BD%B2/#sec-2">前文</a>介绍了Transformer的基本原理和结构。这里做下Attention机制的补充说明： Transformer结构中的q/k/v的Attention机制，其实来源于 <code class="language-plaintext highlighter-rouge">寻址（Addressing）</code> 的概念：给定一个和任务相关的查询Query向量q，通过计算与Key的注意力分布并附加在Value上，得到最终的Attention value。而 <code class="language-plaintext highlighter-rouge">self-Attention</code> 的区别在于，q/k/v的值都是基于自身输入的变换（矩阵乘法得到）。Transformer中的encoder-decoder结构包含了 <code class="language-plaintext highlighter-rouge">self Attention</code> 和 <code class="language-plaintext highlighter-rouge">encoder-decoder-attention</code> ，同时添加了位置编码信息和mask信息（方便双向训练）： <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_160835.png" alt="img" /></p><p>BERT基于Transformer的改进在于：</p><ol><li><p>无监督的训练方式：原始transformer模型训练没有考虑无监督训练，通过Masked LM方法，随机Mask掉15%的词来让BERT进行预测，同时也解决了只有上文信息的问题。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_161841.png" alt="img" /></p><li><p>多任务训练：BERT输入为两个句子，通过分隔符sep进行分割，通过添加segment embedding进行区分。训练中进行句子的选取：50%概率抽连续句子，作为正样本；50%概率抽随机句子，作为负样本，该任务作为问答场景很适用。 BERT的下游任务适配：针对不同的NLP任务有不同的使用方式，具体可见下图。BERT的优点在于：实现了真正的双向，解决了多义性的问题；并行计算，效率很高；迁移性强，易于适配下游任务。 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://cdn.jsdelivr.net/gh/ZhengWG/Imgs_blog//2022-05-03-Bert:%25E8%25BF%2587%25E5%258E%25BB-%25E7%258E%25B0%25E5%259C%25A8-%25E6%259C%25AA%25E6%259D%25A5/BERT:%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5_20220503_163014.png" alt="img" /></p></ol><h1 id="bert后期发展">BERT后期发展<a id="sec-4"></a></h1><p>BERT之后，衍生了各类改进版BERT，具体维度主要包括：改善训练方式、优化模型结构、模型小型化等，以下介绍几个经典的模型：</p><ol><li>XL-Net：主要采用了更好的训练方式：采用AR方式，避免了mask标记位（导致了finetune和pretrain训练不一致带来的误差）；替换transfomer为transfoermer-XL。<li>ELECTRA：提出了新的预训练任务和框架：原始的Mask LM方式替换为判别式的Replaced token detection任务，采用了对抗训练的方式。<li>ERNIE：采用了更好的mask设计；采用了更多的语料（知识类的中文语料）进行预训练。<li>RoBERTA：改进了训练方法：改变mask的方式；丢弃NSP任务；优化超参；采用更大规模的训练数据。<li>ALBERT：重点在于减少内存的网络设计：对Embedding进行因式分解；跨层的参数共享（全连接层和attention层都进行参数共享）；优化了训练方式（去除了部分task和dropout层）。<li>TinyBERT：设计更小的transformer结构，并专门设计了transformer蒸馏的方式保持精度。</ol><h1 id="参考资料">参考资料<a id="sec-5"></a></h1><p><a href="https://mp.weixin.qq.com/s/p16IEzlaDGRNt8h6WkP-dQ">从Word Embedding到BERT模型-自然语言处理中的预训练技术发展史</a></p><p><a href="https://zhuanlan.zhihu.com/p/53682800">nlp中的Attention注意力机制+Transformer详解</a></p><p><a href="https://mp.weixin.qq.com/s/H4at_BDLwZWqlBHLjMZWRQ">一步步理解BERT</a></p><p><a href="https://mp.weixin.qq.com/s/wWlPWYlwZn7s749XhVrdwg">BERT之后的故事</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%AE%97%E6%B3%95%E7%AF%87/'>算法篇</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/%E7%BB%BC%E8%BF%B0/" class="post-tag no-text-decoration" >综述</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Bert:过去-现在-未来 - Johney Zheng&url=https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Bert:过去-现在-未来 - Johney Zheng&u=https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Bert:过去-现在-未来 - Johney Zheng&url=https://www.johneyzheng.top//posts/Bert-%E8%BF%87%E5%8E%BB-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Blog-Agents%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D/">Blog Agents功能介绍</a><li><a href="/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E6%A0%88/">大模型推理技术栈</a><li><a href="/posts/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">模型部署技术概览</a><li><a href="/posts/FlashAttention%E7%B3%BB%E5%88%97%E4%BC%98%E5%8C%96/">FlashAttention系列优化</a><li><a href="/posts/Win10_Ubuntu_installation/">双系统安装(WIN10+Ubuntu16)</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>接下来阅读</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Tracking%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"><div class="card-body"> <span class="timeago small" > May 31, 2020 <i class="unloaded">2020-05-31T15:41:56+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tracking算法综述</h3><div class="text-muted small"><p> 目标跟踪问题定义 目标跟踪算法简述 生成式模型 判别式模型 深度学习方法 目标跟踪问题定义 目标跟踪分为密集跟踪(a series detections)和稀疏跟踪(estimation+common sense)。前者本质上对每一帧进行检测，需要更大的计算量，实时性差。所以，一般目标跟踪问题都是指的稀疏跟踪：对检...</p></div></div></a></div><div class="card"> <a href="/posts/3D%E5%8D%95%E7%9B%AE(mono_3D)%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0/"><div class="card-body"> <span class="timeago small" > Jan 22, 2021 <i class="unloaded">2021-01-22T21:08:39+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>3D单目(mono 3D)目标检测算法综述</h3><div class="text-muted small"><p> 前言 算法调研(相对完善) 2D升3D问题 表达形式(Representation transformation): BEV, Pseudo-Lidar 关键点&amp;&amp;形状 通过2D/3D约束进行距离估计 直接生成3D候选区域(proposal) 关键总结 前言 翻译自:Monocu...</p></div></div></a></div><div class="card"> <a href="/posts/2D%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0(2D_Detection)/"><div class="card-body"> <span class="timeago small" > Jul 25, 2021 <i class="unloaded">2021-07-25T22:25:12+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>2D检测算法综述(2D_Detection)</h3><div class="text-muted small"><p> 前言 公开数据集 经典论文 基本结构 Backbone 超参 Loss 经典检测经典框架 网络加速 通用检测Tricks 部分论文解读Links 前言 update on 4/10/2021: 终于完成了2D_Detection系列，算是对之前自己在这个领域工作的一点回顾，之后自己会不定期更新一些深度学习模型部署的文章以及论文解读的文章 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/" class="btn btn-outline-primary" prompt="较早文章"><p>设计模式-设计原则</p></a> <a href="/posts/Transformer%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2-GPU%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/" class="btn btn-outline-primary" prompt="较新文章"><p>Transformer离线部署-GPU优化策略</p></a></div><div id="comments"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script> <script src='//unpkg.com/valine/dist/Valine.min.js'></script> <script> new Valine({ el: '#comments', app_id: 'IJm2s0GdkzhEOLwVfClrHeWs-gzGzoHsz', app_key: 'Y281bajarkkIGs8p4WmrTkNi', placeholder: '请在下面评论：', visitor: true }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://github.com/username">Johney Zheng</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，否则本网站上的博客文章均由作者根据知识共享许可协议 - 署名标示 4.0（CC BY 4.0）进行授权许可。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/llms/">LLMs</a> <a class="post-tag" href="/tags/%E7%BB%BC%E8%BF%B0/">综述</a> <a class="post-tag" href="/tags/3d/">3D</a> <a class="post-tag" href="/tags/paper-reading/">Paper_Reading</a> <a class="post-tag" href="/tags/c/">C++</a> <a class="post-tag" href="/tags/kaggle/">Kaggle</a> <a class="post-tag" href="/tags/ubuntu/">Ubuntu</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://www.johneyzheng.top/{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
